Some weights of the model checkpoint at microsoft/deberta-xxlarge-v2 were not used when initializing DebertaV2ForMaskedLM: ['deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']
- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-xxlarge-v2 and are newly initialized: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Found cached dataset jigsaw_toxicity_pred (/home/danielezer/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=jigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85)
Copying templates file templates/jigsaw-toxicity-pred-templates.yaml to /opt/conda/lib/python3.7/site-packages/promptsource/templates/jigsaw_toxicity_pred/templates.yaml
Successfully copied templates file
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Running generate with the following arguments:
Run Number: 0Model: 'deberta'
Prompt Number: '0'
Number of Examples: '500'
Dataset Name: 'jigsaw_toxicity_pred'
Dataset Directory: 'jigsaw'
No Data Balance: 'False'
Toxicity Threshold: '0'
Loading model
Loading dataloader
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 243.26it/s]
Loading cached processed dataset at /home/danielezer/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=jigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85/cache-c6571a49730c6301.arrow
Token indices sequence length is longer than the specified maximum sequence length for this model (673 > 512). Running this sequence through the model will result in indexing errors
Balancing Data
Generating hidden states
  0%|          | 0/84 [00:00<?, ?it/s]  1%|          | 1/84 [00:15<21:08, 15.29s/it]  2%|▏         | 2/84 [00:30<20:50, 15.25s/it]  4%|▎         | 3/84 [00:45<20:33, 15.23s/it]  5%|▍         | 4/84 [01:00<20:18, 15.23s/it]  6%|▌         | 5/84 [01:16<20:02, 15.22s/it]  7%|▋         | 6/84 [01:31<19:44, 15.19s/it]  8%|▊         | 7/84 [01:46<19:27, 15.16s/it] 10%|▉         | 8/84 [02:01<19:11, 15.15s/it] 11%|█         | 9/84 [02:16<18:54, 15.13s/it] 12%|█▏        | 10/84 [02:31<18:39, 15.13s/it] 13%|█▎        | 11/84 [02:46<18:25, 15.14s/it] 14%|█▍        | 12/84 [03:02<18:10, 15.14s/it] 15%|█▌        | 13/84 [03:17<17:54, 15.13s/it] 17%|█▋        | 14/84 [03:32<17:38, 15.13s/it] 18%|█▊        | 15/84 [03:47<17:24, 15.13s/it] 19%|█▉        | 16/84 [04:02<17:09, 15.15s/it] 20%|██        | 17/84 [04:17<16:54, 15.14s/it] 21%|██▏       | 18/84 [04:32<16:39, 15.14s/it] 23%|██▎       | 19/84 [04:47<16:23, 15.13s/it] 24%|██▍       | 20/84 [05:03<16:08, 15.14s/it] 25%|██▌       | 21/84 [05:18<15:53, 15.14s/it] 26%|██▌       | 22/84 [05:33<15:38, 15.14s/it] 27%|██▋       | 23/84 [05:48<15:23, 15.14s/it] 29%|██▊       | 24/84 [06:03<15:08, 15.14s/it] 30%|██▉       | 25/84 [06:18<14:53, 15.14s/it] 31%|███       | 26/84 [06:33<14:37, 15.14s/it] 32%|███▏      | 27/84 [06:49<14:22, 15.13s/it] 33%|███▎      | 28/84 [07:04<14:06, 15.12s/it] 35%|███▍      | 29/84 [07:19<13:52, 15.14s/it] 36%|███▌      | 30/84 [07:34<13:37, 15.13s/it] 37%|███▋      | 31/84 [07:49<13:22, 15.14s/it] 38%|███▊      | 32/84 [08:04<13:06, 15.12s/it] 39%|███▉      | 33/84 [08:19<12:51, 15.14s/it] 40%|████      | 34/84 [08:35<12:36, 15.14s/it] 42%|████▏     | 35/84 [08:50<12:21, 15.13s/it] 43%|████▎     | 36/84 [09:05<12:06, 15.13s/it] 44%|████▍     | 37/84 [09:20<11:52, 15.15s/it] 45%|████▌     | 38/84 [09:35<11:36, 15.14s/it] 46%|████▋     | 39/84 [09:50<11:20, 15.12s/it] 48%|████▊     | 40/84 [10:05<11:05, 15.13s/it] 49%|████▉     | 41/84 [10:20<10:50, 15.12s/it] 50%|█████     | 42/84 [10:36<10:34, 15.12s/it] 51%|█████     | 43/84 [10:51<10:19, 15.12s/it] 52%|█████▏    | 44/84 [11:06<10:05, 15.13s/it] 54%|█████▎    | 45/84 [11:21<09:49, 15.12s/it] 55%|█████▍    | 46/84 [11:36<09:35, 15.15s/it] 56%|█████▌    | 47/84 [11:51<09:20, 15.14s/it] 57%|█████▋    | 48/84 [12:06<09:04, 15.14s/it] 58%|█████▊    | 49/84 [12:21<08:49, 15.12s/it] 60%|█████▉    | 50/84 [12:37<08:34, 15.13s/it] 61%|██████    | 51/84 [12:52<08:19, 15.12s/it] 62%|██████▏   | 52/84 [13:07<08:03, 15.12s/it] 63%|██████▎   | 53/84 [13:22<07:48, 15.12s/it] 64%|██████▍   | 54/84 [13:37<07:33, 15.12s/it] 65%|██████▌   | 55/84 [13:52<07:18, 15.12s/it] 67%|██████▋   | 56/84 [14:07<07:03, 15.12s/it] 68%|██████▊   | 57/84 [14:22<06:48, 15.13s/it] 69%|██████▉   | 58/84 [14:38<06:33, 15.12s/it] 70%|███████   | 59/84 [14:53<06:17, 15.12s/it] 71%|███████▏  | 60/84 [15:08<06:02, 15.11s/it] 73%|███████▎  | 61/84 [15:23<05:47, 15.11s/it] 74%|███████▍  | 62/84 [15:38<05:32, 15.13s/it] 75%|███████▌  | 63/84 [15:53<05:17, 15.13s/it] 76%|███████▌  | 64/84 [16:08<05:02, 15.11s/it] 77%|███████▋  | 65/84 [16:23<04:47, 15.12s/it] 79%|███████▊  | 66/84 [16:39<04:32, 15.12s/it] 80%|███████▉  | 67/84 [16:54<04:17, 15.13s/it] 81%|████████  | 68/84 [17:09<04:02, 15.14s/it] 82%|████████▏ | 69/84 [17:24<03:47, 15.15s/it] 83%|████████▎ | 70/84 [17:39<03:31, 15.14s/it] 85%|████████▍ | 71/84 [17:54<03:16, 15.14s/it] 86%|████████▌ | 72/84 [18:09<03:01, 15.14s/it] 87%|████████▋ | 73/84 [18:25<02:46, 15.15s/it] 88%|████████▊ | 74/84 [18:40<02:31, 15.16s/it] 89%|████████▉ | 75/84 [18:55<02:16, 15.16s/it] 90%|█████████ | 76/84 [19:10<02:01, 15.15s/it] 92%|█████████▏| 77/84 [19:25<01:46, 15.16s/it] 93%|█████████▎| 78/84 [19:40<01:30, 15.15s/it] 94%|█████████▍| 79/84 [19:56<01:15, 15.16s/it] 95%|█████████▌| 80/84 [20:11<01:00, 15.17s/it] 96%|█████████▋| 81/84 [20:26<00:45, 15.17s/it] 98%|█████████▊| 82/84 [20:41<00:30, 15.16s/it] 99%|█████████▉| 83/84 [20:56<00:15, 15.14s/it]100%|██████████| 84/84 [21:01<00:00, 12.18s/it]100%|██████████| 84/84 [21:01<00:00, 15.02s/it]
/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,
Some weights of the model checkpoint at microsoft/deberta-xxlarge-v2 were not used when initializing DebertaV2ForMaskedLM: ['deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']
- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-xxlarge-v2 and are newly initialized: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Found cached dataset jigsaw_toxicity_pred (/home/danielezer/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=jigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85)
Saving hidden states
Running evaluate with the following arguments:
Run Number: 0Model: 'deberta'
Prompt Number: '0'
Number of Examples: '500'
Dataset Name: 'jigsaw_toxicity_pred'
Dataset Directory: 'jigsaw'
No Data Balance: 'False'
Toxicity Threshold: '0'
Logistic regression accuracy: 0.792
CCS accuracy: 0.672
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Running generate with the following arguments:
Run Number: 0Model: 'deberta'
Prompt Number: '0'
Number of Examples: '500'
Dataset Name: 'jigsaw_toxicity_pred'
Dataset Directory: 'jigsaw'
No Data Balance: 'True'
Toxicity Threshold: '0'
Loading model
Loading dataloader
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 241.37it/s]
Loading cached processed dataset at /home/danielezer/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=jigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85/cache-c6571a49730c6301.arrow
Token indices sequence length is longer than the specified maximum sequence length for this model (690 > 512). Running this sequence through the model will result in indexing errors
Generating hidden states
  0%|          | 0/84 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
  1%|          | 1/84 [00:15<21:11, 15.32s/it]  2%|▏         | 2/84 [00:30<20:51, 15.26s/it]  4%|▎         | 3/84 [00:45<20:32, 15.22s/it]  5%|▍         | 4/84 [01:00<20:14, 15.19s/it]  6%|▌         | 5/84 [01:15<19:57, 15.16s/it]  7%|▋         | 6/84 [01:31<19:41, 15.14s/it]  8%|▊         | 7/84 [01:46<19:27, 15.17s/it] 10%|▉         | 8/84 [02:01<19:11, 15.16s/it] 11%|█         | 9/84 [02:16<18:57, 15.16s/it] 12%|█▏        | 10/84 [02:31<18:42, 15.17s/it] 13%|█▎        | 11/84 [02:46<18:26, 15.16s/it] 14%|█▍        | 12/84 [03:02<18:10, 15.15s/it] 15%|█▌        | 13/84 [03:17<17:56, 15.16s/it] 17%|█▋        | 14/84 [03:32<17:39, 15.14s/it] 18%|█▊        | 15/84 [03:47<17:24, 15.14s/it] 19%|█▉        | 16/84 [04:02<17:08, 15.13s/it] 20%|██        | 17/84 [04:17<16:53, 15.12s/it] 21%|██▏       | 18/84 [04:32<16:39, 15.14s/it] 23%|██▎       | 19/84 [04:48<16:25, 15.16s/it] 24%|██▍       | 20/84 [05:03<16:09, 15.15s/it] 25%|██▌       | 21/84 [05:18<15:54, 15.15s/it] 26%|██▌       | 22/84 [05:33<15:40, 15.17s/it] 27%|██▋       | 23/84 [05:48<15:25, 15.17s/it] 29%|██▊       | 24/84 [06:03<15:08, 15.15s/it] 30%|██▉       | 25/84 [06:18<14:53, 15.14s/it] 31%|███       | 26/84 [06:34<14:38, 15.15s/it] 32%|███▏      | 27/84 [06:49<14:23, 15.14s/it] 33%|███▎      | 28/84 [07:04<14:08, 15.16s/it] 35%|███▍      | 29/84 [07:19<13:54, 15.17s/it] 36%|███▌      | 30/84 [07:34<13:38, 15.15s/it] 37%|███▋      | 31/84 [07:49<13:23, 15.16s/it] 38%|███▊      | 32/84 [08:05<13:08, 15.16s/it] 39%|███▉      | 33/84 [08:20<12:52, 15.16s/it] 40%|████      | 34/84 [08:35<12:38, 15.18s/it] 42%|████▏     | 35/84 [08:50<12:22, 15.15s/it] 43%|████▎     | 36/84 [09:05<12:06, 15.15s/it] 44%|████▍     | 37/84 [09:20<11:51, 15.14s/it] 45%|████▌     | 38/84 [09:35<11:36, 15.14s/it] 46%|████▋     | 39/84 [09:51<11:22, 15.16s/it] 48%|████▊     | 40/84 [10:06<11:06, 15.15s/it] 49%|████▉     | 41/84 [10:21<10:52, 15.17s/it] 50%|█████     | 42/84 [10:36<10:36, 15.16s/it] 51%|█████     | 43/84 [10:51<10:21, 15.17s/it] 52%|█████▏    | 44/84 [11:06<10:05, 15.15s/it] 54%|█████▎    | 45/84 [11:22<09:50, 15.14s/it] 55%|█████▍    | 46/84 [11:37<09:35, 15.14s/it] 56%|█████▌    | 47/84 [11:52<09:20, 15.14s/it] 57%|█████▋    | 48/84 [12:07<09:04, 15.14s/it] 58%|█████▊    | 49/84 [12:22<08:49, 15.13s/it] 60%|█████▉    | 50/84 [12:37<08:34, 15.12s/it] 61%|██████    | 51/84 [12:52<08:19, 15.13s/it] 62%|██████▏   | 52/84 [13:07<08:04, 15.14s/it] 63%|██████▎   | 53/84 [13:23<07:49, 15.14s/it] 64%|██████▍   | 54/84 [13:38<07:34, 15.14s/it] 65%|██████▌   | 55/84 [13:53<07:19, 15.14s/it] 67%|██████▋   | 56/84 [14:08<07:03, 15.14s/it] 68%|██████▊   | 57/84 [14:23<06:48, 15.14s/it] 69%|██████▉   | 58/84 [14:38<06:33, 15.14s/it] 70%|███████   | 59/84 [14:53<06:18, 15.15s/it] 71%|███████▏  | 60/84 [15:09<06:03, 15.13s/it] 73%|███████▎  | 61/84 [15:24<05:48, 15.13s/it] 74%|███████▍  | 62/84 [15:39<05:32, 15.13s/it] 75%|███████▌  | 63/84 [15:54<05:17, 15.13s/it] 76%|███████▌  | 64/84 [16:09<05:02, 15.12s/it] 77%|███████▋  | 65/84 [16:24<04:47, 15.13s/it] 79%|███████▊  | 66/84 [16:39<04:32, 15.13s/it] 80%|███████▉  | 67/84 [16:54<04:17, 15.13s/it] 81%|████████  | 68/84 [17:10<04:02, 15.13s/it] 82%|████████▏ | 69/84 [17:25<03:46, 15.13s/it] 83%|████████▎ | 70/84 [17:40<03:32, 15.14s/it] 85%|████████▍ | 71/84 [17:55<03:16, 15.14s/it] 86%|████████▌ | 72/84 [18:10<03:01, 15.14s/it] 87%|████████▋ | 73/84 [18:25<02:46, 15.16s/it] 88%|████████▊ | 74/84 [18:40<02:31, 15.14s/it] 89%|████████▉ | 75/84 [18:56<02:16, 15.13s/it] 90%|█████████ | 76/84 [19:11<02:00, 15.11s/it] 92%|█████████▏| 77/84 [19:26<01:45, 15.14s/it] 93%|█████████▎| 78/84 [19:41<01:30, 15.14s/it] 94%|█████████▍| 79/84 [19:56<01:15, 15.14s/it] 95%|█████████▌| 80/84 [20:11<01:00, 15.14s/it] 96%|█████████▋| 81/84 [20:26<00:45, 15.13s/it] 98%|█████████▊| 82/84 [20:42<00:30, 15.13s/it] 99%|█████████▉| 83/84 [20:57<00:15, 15.12s/it]100%|██████████| 84/84 [21:02<00:00, 12.17s/it]100%|██████████| 84/84 [21:02<00:00, 15.03s/it]
Some weights of the model checkpoint at microsoft/deberta-xxlarge-v2 were not used when initializing DebertaV2ForMaskedLM: ['deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']
- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-xxlarge-v2 and are newly initialized: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Found cached dataset jigsaw_toxicity_pred (/home/danielezer/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=jigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85)
Saving hidden states
Running evaluate with the following arguments:
Run Number: 0Model: 'deberta'
Prompt Number: '0'
Number of Examples: '500'
Dataset Name: 'jigsaw_toxicity_pred'
Dataset Directory: 'jigsaw'
No Data Balance: 'True'
Toxicity Threshold: '0'
Logistic regression accuracy: 0.892
CCS accuracy: 0.556
Finished running generate and evaluate with all model configurations
Copying templates file templates/jigsaw-toxicity-pred-templates.yaml to /opt/conda/lib/python3.7/site-packages/promptsource/templates/jigsaw_toxicity_pred/templates.yaml
Successfully copied templates file
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Running generate with the following arguments:
Run Number: 1Model: 'deberta'
Prompt Number: '0'
Number of Examples: '500'
Dataset Name: 'jigsaw_toxicity_pred'
Dataset Directory: 'jigsaw'
No Data Balance: 'False'
Toxicity Threshold: '0'
Loading model
Loading dataloader
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 262.46it/s]
Loading cached processed dataset at /home/danielezer/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=jigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85/cache-c6571a49730c6301.arrow
Token indices sequence length is longer than the specified maximum sequence length for this model (534 > 512). Running this sequence through the model will result in indexing errors
Balancing Data
Generating hidden states
  0%|          | 0/84 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
  1%|          | 1/84 [00:15<21:16, 15.38s/it]  2%|▏         | 2/84 [00:30<20:53, 15.29s/it]  4%|▎         | 3/84 [00:45<20:33, 15.23s/it]  5%|▍         | 4/84 [01:00<20:14, 15.19s/it]  6%|▌         | 5/84 [01:16<19:59, 15.18s/it]  7%|▋         | 6/84 [01:31<19:42, 15.16s/it]  8%|▊         | 7/84 [01:46<19:26, 15.15s/it] 10%|▉         | 8/84 [02:01<19:11, 15.15s/it] 11%|█         | 9/84 [02:16<18:56, 15.15s/it] 12%|█▏        | 10/84 [02:31<18:39, 15.13s/it] 13%|█▎        | 11/84 [02:46<18:24, 15.13s/it] 14%|█▍        | 12/84 [03:01<18:08, 15.12s/it] 15%|█▌        | 13/84 [03:17<17:53, 15.12s/it] 17%|█▋        | 14/84 [03:32<17:38, 15.12s/it] 18%|█▊        | 15/84 [03:47<17:24, 15.14s/it] 19%|█▉        | 16/84 [04:02<17:08, 15.12s/it] 20%|██        | 17/84 [04:17<16:53, 15.12s/it] 21%|██▏       | 18/84 [04:32<16:37, 15.12s/it] 23%|██▎       | 19/84 [04:47<16:22, 15.12s/it] 24%|██▍       | 20/84 [05:02<16:08, 15.13s/it] 25%|██▌       | 21/84 [05:18<15:52, 15.12s/it] 26%|██▌       | 22/84 [05:33<15:37, 15.13s/it] 27%|██▋       | 23/84 [05:48<15:22, 15.13s/it] 29%|██▊       | 24/84 [06:03<15:06, 15.12s/it] 30%|██▉       | 25/84 [06:18<14:51, 15.11s/it] 31%|███       | 26/84 [06:33<14:36, 15.11s/it] 32%|███▏      | 27/84 [06:48<14:21, 15.11s/it] 33%|███▎      | 28/84 [07:03<14:06, 15.11s/it] 35%|███▍      | 29/84 [07:18<13:51, 15.13s/it] 36%|███▌      | 30/84 [07:34<13:36, 15.11s/it] 37%|███▋      | 31/84 [07:49<13:21, 15.11s/it] 38%|███▊      | 32/84 [08:04<13:05, 15.10s/it] 39%|███▉      | 33/84 [08:19<12:51, 15.12s/it] 40%|████      | 34/84 [08:34<12:37, 15.14s/it] 42%|████▏     | 35/84 [08:49<12:21, 15.14s/it] 43%|████▎     | 36/84 [09:04<12:06, 15.13s/it] 44%|████▍     | 37/84 [09:19<11:50, 15.12s/it] 45%|████▌     | 38/84 [09:35<11:36, 15.13s/it] 46%|████▋     | 39/84 [09:50<11:20, 15.13s/it] 48%|████▊     | 40/84 [10:05<11:05, 15.13s/it] 49%|████▉     | 41/84 [10:20<10:51, 15.15s/it] 50%|█████     | 42/84 [10:35<10:35, 15.13s/it] 51%|█████     | 43/84 [10:50<10:21, 15.15s/it] 52%|█████▏    | 44/84 [11:05<10:05, 15.13s/it] 54%|█████▎    | 45/84 [11:21<09:50, 15.13s/it] 55%|█████▍    | 46/84 [11:36<09:34, 15.13s/it] 56%|█████▌    | 47/84 [11:51<09:19, 15.12s/it] 57%|█████▋    | 48/84 [12:06<09:04, 15.12s/it] 58%|█████▊    | 49/84 [12:21<08:49, 15.13s/it] 60%|█████▉    | 50/84 [12:36<08:34, 15.13s/it] 61%|██████    | 51/84 [12:51<08:20, 15.17s/it] 62%|██████▏   | 52/84 [13:07<08:04, 15.15s/it] 63%|██████▎   | 53/84 [13:22<07:49, 15.15s/it] 64%|██████▍   | 54/84 [13:37<07:33, 15.13s/it] 65%|██████▌   | 55/84 [13:52<07:19, 15.15s/it] 67%|██████▋   | 56/84 [14:07<07:04, 15.16s/it] 68%|██████▊   | 57/84 [14:22<06:49, 15.15s/it] 69%|██████▉   | 58/84 [14:37<06:34, 15.16s/it] 70%|███████   | 59/84 [14:53<06:18, 15.15s/it] 71%|███████▏  | 60/84 [15:08<06:04, 15.17s/it] 73%|███████▎  | 61/84 [15:23<05:48, 15.15s/it] 74%|███████▍  | 62/84 [15:38<05:32, 15.13s/it] 75%|███████▌  | 63/84 [15:53<05:17, 15.12s/it] 76%|███████▌  | 64/84 [16:08<05:02, 15.13s/it] 77%|███████▋  | 65/84 [16:23<04:47, 15.12s/it] 79%|███████▊  | 66/84 [16:38<04:32, 15.13s/it] 80%|███████▉  | 67/84 [16:54<04:16, 15.12s/it] 81%|████████  | 68/84 [17:09<04:01, 15.12s/it] 82%|████████▏ | 69/84 [17:24<03:46, 15.12s/it] 83%|████████▎ | 70/84 [17:39<03:31, 15.11s/it] 85%|████████▍ | 71/84 [17:54<03:16, 15.13s/it] 86%|████████▌ | 72/84 [18:09<03:01, 15.15s/it] 87%|████████▋ | 73/84 [18:24<02:46, 15.16s/it] 88%|████████▊ | 74/84 [18:40<02:31, 15.14s/it] 89%|████████▉ | 75/84 [18:55<02:16, 15.14s/it] 90%|█████████ | 76/84 [19:10<02:01, 15.13s/it] 92%|█████████▏| 77/84 [19:25<01:45, 15.12s/it] 93%|█████████▎| 78/84 [19:40<01:30, 15.11s/it] 94%|█████████▍| 79/84 [19:55<01:15, 15.12s/it] 95%|█████████▌| 80/84 [20:10<01:00, 15.13s/it] 96%|█████████▋| 81/84 [20:25<00:45, 15.12s/it] 98%|█████████▊| 82/84 [20:41<00:30, 15.14s/it] 99%|█████████▉| 83/84 [20:56<00:15, 15.13s/it]100%|██████████| 84/84 [21:01<00:00, 12.17s/it]100%|██████████| 84/84 [21:01<00:00, 15.02s/it]
/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,
Some weights of the model checkpoint at microsoft/deberta-xxlarge-v2 were not used when initializing DebertaV2ForMaskedLM: ['deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']
- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-xxlarge-v2 and are newly initialized: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Found cached dataset jigsaw_toxicity_pred (/home/danielezer/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=jigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85)
Saving hidden states
Running evaluate with the following arguments:
Run Number: 1Model: 'deberta'
Prompt Number: '0'
Number of Examples: '500'
Dataset Name: 'jigsaw_toxicity_pred'
Dataset Directory: 'jigsaw'
No Data Balance: 'False'
Toxicity Threshold: '0'
Logistic regression accuracy: 0.816
CCS accuracy: 0.632
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Running generate with the following arguments:
Run Number: 1Model: 'deberta'
Prompt Number: '0'
Number of Examples: '500'
Dataset Name: 'jigsaw_toxicity_pred'
Dataset Directory: 'jigsaw'
No Data Balance: 'True'
Toxicity Threshold: '0'
Loading model
Loading dataloader
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 254.18it/s]
Loading cached processed dataset at /home/danielezer/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=jigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85/cache-c6571a49730c6301.arrow
Token indices sequence length is longer than the specified maximum sequence length for this model (646 > 512). Running this sequence through the model will result in indexing errors
Generating hidden states
  0%|          | 0/84 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
  1%|          | 1/84 [00:15<21:10, 15.31s/it]  2%|▏         | 2/84 [00:30<20:53, 15.29s/it]  4%|▎         | 3/84 [00:45<20:36, 15.26s/it]  5%|▍         | 4/84 [01:01<20:20, 15.25s/it]  6%|▌         | 5/84 [01:16<20:03, 15.23s/it]  7%|▋         | 6/84 [01:31<19:44, 15.19s/it]  8%|▊         | 7/84 [01:46<19:28, 15.18s/it] 10%|▉         | 8/84 [02:01<19:11, 15.16s/it] 11%|█         | 9/84 [02:16<18:56, 15.15s/it] 12%|█▏        | 10/84 [02:31<18:40, 15.15s/it] 13%|█▎        | 11/84 [02:47<18:26, 15.15s/it] 14%|█▍        | 12/84 [03:02<18:10, 15.15s/it] 15%|█▌        | 13/84 [03:17<17:56, 15.16s/it] 17%|█▋        | 14/84 [03:32<17:40, 15.15s/it] 18%|█▊        | 15/84 [03:47<17:25, 15.15s/it] 19%|█▉        | 16/84 [04:02<17:09, 15.14s/it] 20%|██        | 17/84 [04:17<16:53, 15.13s/it] 21%|██▏       | 18/84 [04:33<16:39, 15.14s/it] 23%|██▎       | 19/84 [04:48<16:23, 15.13s/it] 24%|██▍       | 20/84 [05:03<16:07, 15.12s/it] 25%|██▌       | 21/84 [05:18<15:52, 15.12s/it] 26%|██▌       | 22/84 [05:33<15:37, 15.11s/it] 27%|██▋       | 23/84 [05:48<15:22, 15.12s/it] 29%|██▊       | 24/84 [06:03<15:07, 15.12s/it] 30%|██▉       | 25/84 [06:18<14:53, 15.15s/it] 31%|███       | 26/84 [06:34<14:38, 15.14s/it] 32%|███▏      | 27/84 [06:49<14:22, 15.12s/it] 33%|███▎      | 28/84 [07:04<14:07, 15.14s/it] 35%|███▍      | 29/84 [07:19<13:52, 15.13s/it] 36%|███▌      | 30/84 [07:34<13:36, 15.12s/it] 37%|███▋      | 31/84 [07:49<13:22, 15.14s/it] 38%|███▊      | 32/84 [08:04<13:06, 15.13s/it] 39%|███▉      | 33/84 [08:19<12:52, 15.14s/it] 40%|████      | 34/84 [08:35<12:36, 15.14s/it] 42%|████▏     | 35/84 [08:50<12:22, 15.16s/it] 43%|████▎     | 36/84 [09:05<12:07, 15.15s/it] 44%|████▍     | 37/84 [09:20<11:52, 15.15s/it] 45%|████▌     | 38/84 [09:35<11:37, 15.16s/it] 46%|████▋     | 39/84 [09:50<11:21, 15.15s/it] 48%|████▊     | 40/84 [10:06<11:06, 15.15s/it] 49%|████▉     | 41/84 [10:21<10:50, 15.14s/it] 50%|█████     | 42/84 [10:36<10:35, 15.13s/it] 51%|█████     | 43/84 [10:51<10:20, 15.14s/it] 52%|█████▏    | 44/84 [11:06<10:06, 15.16s/it] 54%|█████▎    | 45/84 [11:21<09:50, 15.15s/it] 55%|█████▍    | 46/84 [11:36<09:35, 15.16s/it] 56%|█████▌    | 47/84 [11:52<09:20, 15.14s/it] 57%|█████▋    | 48/84 [12:07<09:04, 15.14s/it] 58%|█████▊    | 49/84 [12:22<08:49, 15.12s/it] 60%|█████▉    | 50/84 [12:37<08:34, 15.14s/it] 61%|██████    | 51/84 [12:52<08:19, 15.13s/it] 62%|██████▏   | 52/84 [13:07<08:04, 15.15s/it] 63%|██████▎   | 53/84 [13:22<07:50, 15.16s/it] 64%|██████▍   | 54/84 [13:38<07:34, 15.14s/it] 65%|██████▌   | 55/84 [13:53<07:19, 15.14s/it] 67%|██████▋   | 56/84 [14:08<07:04, 15.15s/it] 68%|██████▊   | 57/84 [14:23<06:48, 15.13s/it] 69%|██████▉   | 58/84 [14:38<06:33, 15.15s/it] 70%|███████   | 59/84 [14:53<06:19, 15.16s/it] 71%|███████▏  | 60/84 [15:08<06:03, 15.15s/it] 73%|███████▎  | 61/84 [15:24<05:48, 15.15s/it] 74%|███████▍  | 62/84 [15:39<05:33, 15.15s/it] 75%|███████▌  | 63/84 [15:54<05:17, 15.13s/it] 76%|███████▌  | 64/84 [16:09<05:02, 15.14s/it] 77%|███████▋  | 65/84 [16:24<04:47, 15.13s/it] 79%|███████▊  | 66/84 [16:39<04:32, 15.14s/it] 80%|███████▉  | 67/84 [16:54<04:17, 15.15s/it] 81%|████████  | 68/84 [17:10<04:02, 15.15s/it] 82%|████████▏ | 69/84 [17:25<03:47, 15.14s/it] 83%|████████▎ | 70/84 [17:40<03:32, 15.15s/it] 85%|████████▍ | 71/84 [17:55<03:16, 15.14s/it] 86%|████████▌ | 72/84 [18:10<03:01, 15.16s/it] 87%|████████▋ | 73/84 [18:25<02:46, 15.16s/it] 88%|████████▊ | 74/84 [18:40<02:31, 15.15s/it] 89%|████████▉ | 75/84 [18:56<02:16, 15.15s/it] 90%|█████████ | 76/84 [19:11<02:01, 15.15s/it] 92%|█████████▏| 77/84 [19:26<01:46, 15.15s/it] 93%|█████████▎| 78/84 [19:41<01:30, 15.16s/it] 94%|█████████▍| 79/84 [19:56<01:15, 15.15s/it] 95%|█████████▌| 80/84 [20:11<01:00, 15.13s/it] 96%|█████████▋| 81/84 [20:26<00:45, 15.13s/it] 98%|█████████▊| 82/84 [20:42<00:30, 15.13s/it] 99%|█████████▉| 83/84 [20:57<00:15, 15.12s/it]100%|██████████| 84/84 [21:02<00:00, 12.16s/it]100%|██████████| 84/84 [21:02<00:00, 15.03s/it]
Some weights of the model checkpoint at microsoft/deberta-xxlarge-v2 were not used when initializing DebertaV2ForMaskedLM: ['deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']
- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-xxlarge-v2 and are newly initialized: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Found cached dataset jigsaw_toxicity_pred (/home/danielezer/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=jigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85)
Saving hidden states
Running evaluate with the following arguments:
Run Number: 1Model: 'deberta'
Prompt Number: '0'
Number of Examples: '500'
Dataset Name: 'jigsaw_toxicity_pred'
Dataset Directory: 'jigsaw'
No Data Balance: 'True'
Toxicity Threshold: '0'
Logistic regression accuracy: 0.876
CCS accuracy: 0.508
Finished running generate and evaluate with all model configurations
Copying templates file templates/jigsaw-toxicity-pred-templates.yaml to /opt/conda/lib/python3.7/site-packages/promptsource/templates/jigsaw_toxicity_pred/templates.yaml
Successfully copied templates file
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Running generate with the following arguments:
Run Number: 2Model: 'deberta'
Prompt Number: '0'
Number of Examples: '500'
Dataset Name: 'jigsaw_toxicity_pred'
Dataset Directory: 'jigsaw'
No Data Balance: 'False'
Toxicity Threshold: '0'
Loading model
Loading dataloader
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 253.35it/s]
Loading cached processed dataset at /home/danielezer/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=jigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85/cache-c6571a49730c6301.arrow
Token indices sequence length is longer than the specified maximum sequence length for this model (644 > 512). Running this sequence through the model will result in indexing errors
Balancing Data
Generating hidden states
  0%|          | 0/84 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
  1%|          | 1/84 [00:15<21:13, 15.35s/it]  2%|▏         | 2/84 [00:30<20:53, 15.28s/it]  4%|▎         | 3/84 [00:45<20:34, 15.24s/it]  5%|▍         | 4/84 [01:00<20:15, 15.19s/it]  6%|▌         | 5/84 [01:16<20:01, 15.21s/it]  7%|▋         | 6/84 [01:31<19:44, 15.19s/it]  8%|▊         | 7/84 [01:46<19:28, 15.18s/it] 10%|▉         | 8/84 [02:01<19:12, 15.17s/it] 11%|█         | 9/84 [02:16<18:59, 15.19s/it] 12%|█▏        | 10/84 [02:31<18:41, 15.16s/it] 13%|█▎        | 11/84 [02:46<18:25, 15.14s/it] 14%|█▍        | 12/84 [03:02<18:10, 15.15s/it] 15%|█▌        | 13/84 [03:17<17:56, 15.17s/it] 17%|█▋        | 14/84 [03:32<17:41, 15.17s/it] 18%|█▊        | 15/84 [03:47<17:26, 15.17s/it] 19%|█▉        | 16/84 [04:02<17:10, 15.16s/it] 20%|██        | 17/84 [04:17<16:54, 15.15s/it] 21%|██▏       | 18/84 [04:33<16:39, 15.14s/it] 23%|██▎       | 19/84 [04:48<16:23, 15.13s/it] 24%|██▍       | 20/84 [05:03<16:08, 15.13s/it] 25%|██▌       | 21/84 [05:18<15:53, 15.13s/it] 26%|██▌       | 22/84 [05:33<15:38, 15.14s/it] 27%|██▋       | 23/84 [05:48<15:22, 15.13s/it] 29%|██▊       | 24/84 [06:03<15:07, 15.12s/it] 30%|██▉       | 25/84 [06:18<14:52, 15.13s/it] 31%|███       | 26/84 [06:34<14:38, 15.15s/it] 32%|███▏      | 27/84 [06:49<14:23, 15.15s/it] 33%|███▎      | 28/84 [07:04<14:07, 15.13s/it] 35%|███▍      | 29/84 [07:19<13:51, 15.11s/it] 36%|███▌      | 30/84 [07:34<13:36, 15.13s/it] 37%|███▋      | 31/84 [07:49<13:21, 15.13s/it] 38%|███▊      | 32/84 [08:04<13:07, 15.14s/it] 39%|███▉      | 33/84 [08:20<12:52, 15.15s/it] 40%|████      | 34/84 [08:35<12:37, 15.14s/it] 42%|████▏     | 35/84 [08:50<12:21, 15.13s/it] 43%|████▎     | 36/84 [09:05<12:06, 15.13s/it] 44%|████▍     | 37/84 [09:20<11:51, 15.14s/it] 45%|████▌     | 38/84 [09:35<11:36, 15.14s/it] 46%|████▋     | 39/84 [09:50<11:21, 15.14s/it] 48%|████▊     | 40/84 [10:06<11:06, 15.14s/it] 49%|████▉     | 41/84 [10:21<10:50, 15.13s/it] 50%|█████     | 42/84 [10:36<10:35, 15.13s/it] 51%|█████     | 43/84 [10:51<10:19, 15.12s/it] 52%|█████▏    | 44/84 [11:06<10:04, 15.11s/it] 54%|█████▎    | 45/84 [11:21<09:49, 15.11s/it] 55%|█████▍    | 46/84 [11:36<09:33, 15.11s/it] 56%|█████▌    | 47/84 [11:51<09:19, 15.11s/it] 57%|█████▋    | 48/84 [12:06<09:04, 15.13s/it] 58%|█████▊    | 49/84 [12:22<08:49, 15.13s/it] 60%|█████▉    | 50/84 [12:37<08:34, 15.13s/it] 61%|██████    | 51/84 [12:52<08:18, 15.12s/it] 62%|██████▏   | 52/84 [13:07<08:03, 15.11s/it] 63%|██████▎   | 53/84 [13:22<07:48, 15.12s/it] 64%|██████▍   | 54/84 [13:37<07:33, 15.13s/it] 65%|██████▌   | 55/84 [13:52<07:18, 15.13s/it] 67%|██████▋   | 56/84 [14:08<07:03, 15.14s/it] 68%|██████▊   | 57/84 [14:23<06:48, 15.12s/it] 69%|██████▉   | 58/84 [14:38<06:32, 15.11s/it] 70%|███████   | 59/84 [14:53<06:17, 15.11s/it] 71%|███████▏  | 60/84 [15:08<06:02, 15.11s/it] 73%|███████▎  | 61/84 [15:23<05:47, 15.12s/it] 74%|███████▍  | 62/84 [15:38<05:32, 15.11s/it] 75%|███████▌  | 63/84 [15:53<05:17, 15.13s/it] 76%|███████▌  | 64/84 [16:08<05:02, 15.13s/it] 77%|███████▋  | 65/84 [16:24<04:47, 15.13s/it] 79%|███████▊  | 66/84 [16:39<04:32, 15.14s/it] 80%|███████▉  | 67/84 [16:54<04:17, 15.13s/it] 81%|████████  | 68/84 [17:09<04:01, 15.12s/it] 82%|████████▏ | 69/84 [17:24<03:46, 15.12s/it] 83%|████████▎ | 70/84 [17:39<03:31, 15.13s/it] 85%|████████▍ | 71/84 [17:54<03:16, 15.14s/it] 86%|████████▌ | 72/84 [18:09<03:01, 15.13s/it] 87%|████████▋ | 73/84 [18:25<02:46, 15.13s/it] 88%|████████▊ | 74/84 [18:40<02:31, 15.14s/it] 89%|████████▉ | 75/84 [18:55<02:16, 15.13s/it] 90%|█████████ | 76/84 [19:10<02:00, 15.12s/it] 92%|█████████▏| 77/84 [19:25<01:45, 15.12s/it] 93%|█████████▎| 78/84 [19:40<01:30, 15.11s/it] 94%|█████████▍| 79/84 [19:55<01:15, 15.11s/it] 95%|█████████▌| 80/84 [20:10<01:00, 15.12s/it] 96%|█████████▋| 81/84 [20:26<00:45, 15.12s/it] 98%|█████████▊| 82/84 [20:41<00:30, 15.11s/it] 99%|█████████▉| 83/84 [20:56<00:15, 15.12s/it]100%|██████████| 84/84 [21:01<00:00, 12.16s/it]100%|██████████| 84/84 [21:01<00:00, 15.02s/it]
Some weights of the model checkpoint at microsoft/deberta-xxlarge-v2 were not used when initializing DebertaV2ForMaskedLM: ['deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']
- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-xxlarge-v2 and are newly initialized: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Found cached dataset jigsaw_toxicity_pred (/home/danielezer/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=jigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85)
Saving hidden states
Running evaluate with the following arguments:
Run Number: 2Model: 'deberta'
Prompt Number: '0'
Number of Examples: '500'
Dataset Name: 'jigsaw_toxicity_pred'
Dataset Directory: 'jigsaw'
No Data Balance: 'False'
Toxicity Threshold: '0'
Logistic regression accuracy: 0.808
CCS accuracy: 0.688
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Running generate with the following arguments:
Run Number: 2Model: 'deberta'
Prompt Number: '0'
Number of Examples: '500'
Dataset Name: 'jigsaw_toxicity_pred'
Dataset Directory: 'jigsaw'
No Data Balance: 'True'
Toxicity Threshold: '0'
Loading model
Loading dataloader
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 262.75it/s]
Loading cached processed dataset at /home/danielezer/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=jigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85/cache-c6571a49730c6301.arrow
Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors
Generating hidden states
  0%|          | 0/84 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
  1%|          | 1/84 [00:15<21:12, 15.33s/it]  2%|▏         | 2/84 [00:30<20:49, 15.23s/it]  4%|▎         | 3/84 [00:45<20:32, 15.21s/it]  5%|▍         | 4/84 [01:00<20:15, 15.19s/it]  6%|▌         | 5/84 [01:15<19:57, 15.16s/it]  7%|▋         | 6/84 [01:31<19:43, 15.17s/it]  8%|▊         | 7/84 [01:46<19:27, 15.16s/it] 10%|▉         | 8/84 [02:01<19:11, 15.15s/it] 11%|█         | 9/84 [02:16<18:55, 15.14s/it] 12%|█▏        | 10/84 [02:31<18:41, 15.15s/it] 13%|█▎        | 11/84 [02:46<18:25, 15.15s/it] 14%|█▍        | 12/84 [03:01<18:10, 15.15s/it] 15%|█▌        | 13/84 [03:17<17:56, 15.16s/it] 17%|█▋        | 14/84 [03:32<17:41, 15.16s/it] 18%|█▊        | 15/84 [03:47<17:26, 15.16s/it] 19%|█▉        | 16/84 [04:02<17:10, 15.15s/it] 20%|██        | 17/84 [04:17<16:54, 15.14s/it] 21%|██▏       | 18/84 [04:32<16:39, 15.15s/it] 23%|██▎       | 19/84 [04:48<16:25, 15.15s/it] 24%|██▍       | 20/84 [05:03<16:09, 15.15s/it] 25%|██▌       | 21/84 [05:18<15:55, 15.16s/it] 26%|██▌       | 22/84 [05:33<15:40, 15.16s/it] 27%|██▋       | 23/84 [05:48<15:25, 15.17s/it] 29%|██▊       | 24/84 [06:03<15:09, 15.15s/it] 30%|██▉       | 25/84 [06:19<14:53, 15.15s/it] 31%|███       | 26/84 [06:34<14:38, 15.15s/it] 32%|███▏      | 27/84 [06:49<14:24, 15.17s/it] 33%|███▎      | 28/84 [07:04<14:09, 15.17s/it] 35%|███▍      | 29/84 [07:19<13:53, 15.15s/it] 36%|███▌      | 30/84 [07:34<13:37, 15.14s/it] 37%|███▋      | 31/84 [07:49<13:22, 15.13s/it] 38%|███▊      | 32/84 [08:05<13:07, 15.15s/it] 39%|███▉      | 33/84 [08:20<12:51, 15.14s/it] 40%|████      | 34/84 [08:35<12:37, 15.14s/it] 42%|████▏     | 35/84 [08:50<12:22, 15.15s/it] 43%|████▎     | 36/84 [09:05<12:07, 15.15s/it] 44%|████▍     | 37/84 [09:20<11:51, 15.14s/it] 45%|████▌     | 38/84 [09:35<11:36, 15.13s/it] 46%|████▋     | 39/84 [09:51<11:22, 15.16s/it] 48%|████▊     | 40/84 [10:06<11:07, 15.16s/it] 49%|████▉     | 41/84 [10:21<10:51, 15.16s/it] 50%|█████     | 42/84 [10:36<10:36, 15.16s/it] 51%|█████     | 43/84 [10:51<10:21, 15.17s/it] 52%|█████▏    | 44/84 [11:06<10:05, 15.15s/it] 54%|█████▎    | 45/84 [11:22<09:50, 15.14s/it] 55%|█████▍    | 46/84 [11:37<09:35, 15.14s/it] 56%|█████▌    | 47/84 [11:52<09:20, 15.14s/it] 57%|█████▋    | 48/84 [12:07<09:04, 15.13s/it] 58%|█████▊    | 49/84 [12:22<08:49, 15.13s/it] 60%|█████▉    | 50/84 [12:37<08:34, 15.12s/it] 61%|██████    | 51/84 [12:52<08:19, 15.14s/it] 62%|██████▏   | 52/84 [13:07<08:04, 15.13s/it] 63%|██████▎   | 53/84 [13:23<07:49, 15.14s/it] 64%|██████▍   | 54/84 [13:38<07:34, 15.15s/it] 65%|██████▌   | 55/84 [13:53<07:19, 15.16s/it] 67%|██████▋   | 56/84 [14:08<07:04, 15.16s/it] 68%|██████▊   | 57/84 [14:23<06:49, 15.15s/it] 69%|██████▉   | 58/84 [14:38<06:33, 15.14s/it] 70%|███████   | 59/84 [14:54<06:18, 15.15s/it] 71%|███████▏  | 60/84 [15:09<06:03, 15.14s/it] 73%|███████▎  | 61/84 [15:24<05:48, 15.15s/it] 74%|███████▍  | 62/84 [15:39<05:32, 15.13s/it] 75%|███████▌  | 63/84 [15:54<05:17, 15.13s/it] 76%|███████▌  | 64/84 [16:09<05:02, 15.14s/it] 77%|███████▋  | 65/84 [16:24<04:48, 15.16s/it] 79%|███████▊  | 66/84 [16:40<04:32, 15.15s/it] 80%|███████▉  | 67/84 [16:55<04:17, 15.14s/it] 81%|████████  | 68/84 [17:10<04:02, 15.15s/it] 82%|████████▏ | 69/84 [17:25<03:47, 15.16s/it] 83%|████████▎ | 70/84 [17:40<03:31, 15.14s/it] 85%|████████▍ | 71/84 [17:55<03:16, 15.14s/it] 86%|████████▌ | 72/84 [18:10<03:01, 15.14s/it] 87%|████████▋ | 73/84 [18:26<02:46, 15.14s/it] 88%|████████▊ | 74/84 [18:41<02:31, 15.13s/it] 89%|████████▉ | 75/84 [18:56<02:16, 15.13s/it] 90%|█████████ | 76/84 [19:11<02:01, 15.14s/it] 92%|█████████▏| 77/84 [19:26<01:46, 15.15s/it] 93%|█████████▎| 78/84 [19:41<01:30, 15.15s/it] 94%|█████████▍| 79/84 [19:56<01:15, 15.14s/it] 95%|█████████▌| 80/84 [20:12<01:00, 15.16s/it] 96%|█████████▋| 81/84 [20:27<00:45, 15.15s/it] 98%|█████████▊| 82/84 [20:42<00:30, 15.14s/it] 99%|█████████▉| 83/84 [20:57<00:15, 15.16s/it]100%|██████████| 84/84 [21:02<00:00, 12.20s/it]100%|██████████| 84/84 [21:02<00:00, 15.03s/it]
Some weights of the model checkpoint at microsoft/deberta-xxlarge-v2 were not used when initializing DebertaV2ForMaskedLM: ['deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']
- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-xxlarge-v2 and are newly initialized: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Found cached dataset jigsaw_toxicity_pred (/home/danielezer/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=jigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85)
Saving hidden states
Running evaluate with the following arguments:
Run Number: 2Model: 'deberta'
Prompt Number: '0'
Number of Examples: '500'
Dataset Name: 'jigsaw_toxicity_pred'
Dataset Directory: 'jigsaw'
No Data Balance: 'True'
Toxicity Threshold: '0'
Logistic regression accuracy: 0.876
CCS accuracy: 0.5680000000000001
Finished running generate and evaluate with all model configurations
Copying templates file templates/jigsaw-toxicity-pred-templates.yaml to /opt/conda/lib/python3.7/site-packages/promptsource/templates/jigsaw_toxicity_pred/templates.yaml
Successfully copied templates file
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Running generate with the following arguments:
Run Number: 3Model: 'deberta'
Prompt Number: '0'
Number of Examples: '500'
Dataset Name: 'jigsaw_toxicity_pred'
Dataset Directory: 'jigsaw'
No Data Balance: 'False'
Toxicity Threshold: '0'
Loading model
Loading dataloader
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 257.49it/s]
Loading cached processed dataset at /home/danielezer/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=jigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85/cache-c6571a49730c6301.arrow
Token indices sequence length is longer than the specified maximum sequence length for this model (732 > 512). Running this sequence through the model will result in indexing errors
Balancing Data
Generating hidden states
  0%|          | 0/84 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
  1%|          | 1/84 [00:15<21:20, 15.43s/it]  2%|▏         | 2/84 [00:30<20:52, 15.27s/it]  4%|▎         | 3/84 [00:45<20:33, 15.22s/it]  5%|▍         | 4/84 [01:00<20:16, 15.21s/it]  6%|▌         | 5/84 [01:16<20:01, 15.21s/it]  7%|▋         | 6/84 [01:31<19:44, 15.19s/it]  8%|▊         | 7/84 [01:46<19:27, 15.17s/it] 10%|▉         | 8/84 [02:01<19:11, 15.16s/it] 11%|█         | 9/84 [02:16<18:56, 15.16s/it] 12%|█▏        | 10/84 [02:31<18:40, 15.14s/it] 13%|█▎        | 11/84 [02:46<18:23, 15.11s/it] 14%|█▍        | 12/84 [03:02<18:09, 15.14s/it] 15%|█▌        | 13/84 [03:17<17:55, 15.15s/it] 17%|█▋        | 14/84 [03:32<17:40, 15.15s/it] 18%|█▊        | 15/84 [03:47<17:24, 15.14s/it] 19%|█▉        | 16/84 [04:02<17:09, 15.14s/it] 20%|██        | 17/84 [04:17<16:54, 15.14s/it] 21%|██▏       | 18/84 [04:32<16:39, 15.14s/it] 23%|██▎       | 19/84 [04:48<16:24, 15.14s/it] 24%|██▍       | 20/84 [05:03<16:08, 15.13s/it] 25%|██▌       | 21/84 [05:18<15:52, 15.12s/it] 26%|██▌       | 22/84 [05:33<15:38, 15.13s/it] 27%|██▋       | 23/84 [05:48<15:23, 15.14s/it] 29%|██▊       | 24/84 [06:03<15:08, 15.14s/it] 30%|██▉       | 25/84 [06:18<14:53, 15.15s/it] 31%|███       | 26/84 [06:34<14:38, 15.14s/it] 32%|███▏      | 27/84 [06:49<14:22, 15.13s/it] 33%|███▎      | 28/84 [07:04<14:07, 15.13s/it] 35%|███▍      | 29/84 [07:19<13:52, 15.13s/it] 36%|███▌      | 30/84 [07:34<13:36, 15.13s/it] 37%|███▋      | 31/84 [07:49<13:22, 15.13s/it] 38%|███▊      | 32/84 [08:04<13:06, 15.13s/it] 39%|███▉      | 33/84 [08:19<12:51, 15.12s/it] 40%|████      | 34/84 [08:34<12:35, 15.12s/it] 42%|████▏     | 35/84 [08:50<12:20, 15.11s/it] 43%|████▎     | 36/84 [09:05<12:06, 15.13s/it] 44%|████▍     | 37/84 [09:20<11:50, 15.12s/it] 45%|████▌     | 38/84 [09:35<11:35, 15.12s/it] 46%|████▋     | 39/84 [09:50<11:20, 15.12s/it] 48%|████▊     | 40/84 [10:05<11:05, 15.12s/it] 49%|████▉     | 41/84 [10:20<10:50, 15.12s/it] 50%|█████     | 42/84 [10:36<10:35, 15.13s/it] 51%|█████     | 43/84 [10:51<10:20, 15.14s/it] 52%|█████▏    | 44/84 [11:06<10:06, 15.17s/it] 54%|█████▎    | 45/84 [11:21<09:51, 15.17s/it] 55%|█████▍    | 46/84 [11:36<09:35, 15.15s/it] 56%|█████▌    | 47/84 [11:51<09:20, 15.15s/it] 57%|█████▋    | 48/84 [12:06<09:04, 15.12s/it] 58%|█████▊    | 49/84 [12:22<08:49, 15.12s/it] 60%|█████▉    | 50/84 [12:37<08:33, 15.11s/it] 61%|██████    | 51/84 [12:52<08:19, 15.13s/it] 62%|██████▏   | 52/84 [13:07<08:04, 15.13s/it] 63%|██████▎   | 53/84 [13:22<07:49, 15.14s/it] 64%|██████▍   | 54/84 [13:37<07:34, 15.14s/it] 65%|██████▌   | 55/84 [13:52<07:18, 15.13s/it] 67%|██████▋   | 56/84 [14:07<07:03, 15.12s/it] 68%|██████▊   | 57/84 [14:23<06:48, 15.12s/it] 69%|██████▉   | 58/84 [14:38<06:33, 15.12s/it] 70%|███████   | 59/84 [14:53<06:18, 15.13s/it] 71%|███████▏  | 60/84 [15:08<06:02, 15.12s/it] 73%|███████▎  | 61/84 [15:23<05:47, 15.12s/it] 74%|███████▍  | 62/84 [15:38<05:32, 15.13s/it] 75%|███████▌  | 63/84 [15:53<05:17, 15.14s/it] 76%|███████▌  | 64/84 [16:08<05:02, 15.14s/it] 77%|███████▋  | 65/84 [16:24<04:47, 15.13s/it] 79%|███████▊  | 66/84 [16:39<04:32, 15.12s/it] 80%|███████▉  | 67/84 [16:54<04:16, 15.11s/it] 81%|████████  | 68/84 [17:09<04:01, 15.12s/it] 82%|████████▏ | 69/84 [17:24<03:46, 15.12s/it] 83%|████████▎ | 70/84 [17:39<03:31, 15.11s/it] 85%|████████▍ | 71/84 [17:54<03:16, 15.12s/it] 86%|████████▌ | 72/84 [18:09<03:01, 15.14s/it] 87%|████████▋ | 73/84 [18:25<02:46, 15.14s/it] 88%|████████▊ | 74/84 [18:40<02:31, 15.14s/it] 89%|████████▉ | 75/84 [18:55<02:16, 15.14s/it] 90%|█████████ | 76/84 [19:10<02:01, 15.13s/it] 92%|█████████▏| 77/84 [19:25<01:45, 15.12s/it] 93%|█████████▎| 78/84 [19:40<01:30, 15.11s/it] 94%|█████████▍| 79/84 [19:55<01:15, 15.10s/it] 95%|█████████▌| 80/84 [20:10<01:00, 15.11s/it] 96%|█████████▋| 81/84 [20:25<00:45, 15.11s/it] 98%|█████████▊| 82/84 [20:41<00:30, 15.12s/it] 99%|█████████▉| 83/84 [20:56<00:15, 15.13s/it]100%|██████████| 84/84 [21:01<00:00, 12.17s/it]100%|██████████| 84/84 [21:01<00:00, 15.02s/it]
/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,
Some weights of the model checkpoint at microsoft/deberta-xxlarge-v2 were not used when initializing DebertaV2ForMaskedLM: ['deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']
- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-xxlarge-v2 and are newly initialized: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Found cached dataset jigsaw_toxicity_pred (/home/danielezer/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=jigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85)
Saving hidden states
Running evaluate with the following arguments:
Run Number: 3Model: 'deberta'
Prompt Number: '0'
Number of Examples: '500'
Dataset Name: 'jigsaw_toxicity_pred'
Dataset Directory: 'jigsaw'
No Data Balance: 'False'
Toxicity Threshold: '0'
Logistic regression accuracy: 0.8
CCS accuracy: 0.5800000000000001
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Running generate with the following arguments:
Run Number: 3Model: 'deberta'
Prompt Number: '0'
Number of Examples: '500'
Dataset Name: 'jigsaw_toxicity_pred'
Dataset Directory: 'jigsaw'
No Data Balance: 'True'
Toxicity Threshold: '0'
Loading model
Loading dataloader
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 237.72it/s]
Loading cached processed dataset at /home/danielezer/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=jigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85/cache-c6571a49730c6301.arrow
Token indices sequence length is longer than the specified maximum sequence length for this model (526 > 512). Running this sequence through the model will result in indexing errors
Generating hidden states
  0%|          | 0/84 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
  1%|          | 1/84 [00:15<21:11, 15.32s/it]  2%|▏         | 2/84 [00:30<20:51, 15.27s/it]  4%|▎         | 3/84 [00:45<20:34, 15.24s/it]  5%|▍         | 4/84 [01:00<20:15, 15.19s/it]  6%|▌         | 5/84 [01:16<19:59, 15.18s/it]  7%|▋         | 6/84 [01:31<19:44, 15.19s/it]  8%|▊         | 7/84 [01:46<19:27, 15.16s/it] 10%|▉         | 8/84 [02:01<19:13, 15.18s/it] 11%|█         | 9/84 [02:16<18:58, 15.18s/it] 12%|█▏        | 10/84 [02:31<18:43, 15.18s/it] 13%|█▎        | 11/84 [02:47<18:27, 15.17s/it] 14%|█▍        | 12/84 [03:02<18:11, 15.16s/it] 15%|█▌        | 13/84 [03:17<17:55, 15.15s/it] 17%|█▋        | 14/84 [03:32<17:41, 15.16s/it] 18%|█▊        | 15/84 [03:47<17:26, 15.17s/it] 19%|█▉        | 16/84 [04:02<17:11, 15.17s/it] 20%|██        | 17/84 [04:17<16:54, 15.15s/it] 21%|██▏       | 18/84 [04:33<16:39, 15.14s/it] 23%|██▎       | 19/84 [04:48<16:24, 15.14s/it] 24%|██▍       | 20/84 [05:03<16:09, 15.14s/it] 25%|██▌       | 21/84 [05:18<15:54, 15.15s/it] 26%|██▌       | 22/84 [05:33<15:38, 15.14s/it] 27%|██▋       | 23/84 [05:48<15:23, 15.14s/it] 29%|██▊       | 24/84 [06:03<15:08, 15.15s/it] 30%|██▉       | 25/84 [06:19<14:54, 15.15s/it] 31%|███       | 26/84 [06:34<14:38, 15.14s/it] 32%|███▏      | 27/84 [06:49<14:23, 15.15s/it] 33%|███▎      | 28/84 [07:04<14:08, 15.16s/it] 35%|███▍      | 29/84 [07:19<13:53, 15.16s/it] 36%|███▌      | 30/84 [07:34<13:38, 15.16s/it] 37%|███▋      | 31/84 [07:50<13:22, 15.14s/it] 38%|███▊      | 32/84 [08:05<13:08, 15.16s/it] 39%|███▉      | 33/84 [08:20<12:53, 15.16s/it] 40%|████      | 34/84 [08:35<12:37, 15.15s/it] 42%|████▏     | 35/84 [08:50<12:22, 15.15s/it] 43%|████▎     | 36/84 [09:05<12:07, 15.16s/it] 44%|████▍     | 37/84 [09:20<11:51, 15.15s/it] 45%|████▌     | 38/84 [09:36<11:37, 15.16s/it] 46%|████▋     | 39/84 [09:51<11:21, 15.15s/it] 48%|████▊     | 40/84 [10:06<11:06, 15.15s/it] 49%|████▉     | 41/84 [10:21<10:51, 15.14s/it] 50%|█████     | 42/84 [10:36<10:35, 15.14s/it] 51%|█████     | 43/84 [10:51<10:20, 15.14s/it] 52%|█████▏    | 44/84 [11:06<10:05, 15.14s/it] 54%|█████▎    | 45/84 [11:22<09:51, 15.16s/it] 55%|█████▍    | 46/84 [11:37<09:36, 15.16s/it] 56%|█████▌    | 47/84 [11:52<09:20, 15.15s/it] 57%|█████▋    | 48/84 [12:07<09:05, 15.15s/it] 58%|█████▊    | 49/84 [12:22<08:50, 15.16s/it] 60%|█████▉    | 50/84 [12:37<08:35, 15.15s/it] 61%|██████    | 51/84 [12:53<08:20, 15.15s/it] 62%|██████▏   | 52/84 [13:08<08:04, 15.15s/it] 63%|██████▎   | 53/84 [13:23<07:50, 15.16s/it] 64%|██████▍   | 54/84 [13:38<07:34, 15.15s/it] 65%|██████▌   | 55/84 [13:53<07:19, 15.15s/it] 67%|██████▋   | 56/84 [14:08<07:03, 15.14s/it] 68%|██████▊   | 57/84 [14:24<06:49, 15.15s/it] 69%|██████▉   | 58/84 [14:39<06:33, 15.15s/it] 70%|███████   | 59/84 [14:54<06:18, 15.15s/it] 71%|███████▏  | 60/84 [15:09<06:03, 15.15s/it] 73%|███████▎  | 61/84 [15:24<05:48, 15.15s/it] 74%|███████▍  | 62/84 [15:39<05:33, 15.15s/it] 75%|███████▌  | 63/84 [15:54<05:18, 15.15s/it] 76%|███████▌  | 64/84 [16:10<05:03, 15.15s/it] 77%|███████▋  | 65/84 [16:25<04:47, 15.15s/it] 79%|███████▊  | 66/84 [16:40<04:32, 15.16s/it] 80%|███████▉  | 67/84 [16:55<04:17, 15.15s/it] 81%|████████  | 68/84 [17:10<04:02, 15.14s/it] 82%|████████▏ | 69/84 [17:25<03:46, 15.10s/it] 83%|████████▎ | 70/84 [17:40<03:31, 15.11s/it] 85%|████████▍ | 71/84 [17:55<03:16, 15.12s/it] 86%|████████▌ | 72/84 [18:11<03:01, 15.12s/it] 87%|████████▋ | 73/84 [18:26<02:46, 15.13s/it] 88%|████████▊ | 74/84 [18:41<02:31, 15.12s/it] 89%|████████▉ | 75/84 [18:56<02:16, 15.15s/it] 90%|█████████ | 76/84 [19:11<02:01, 15.14s/it] 92%|█████████▏| 77/84 [19:26<01:45, 15.13s/it] 93%|█████████▎| 78/84 [19:41<01:30, 15.13s/it] 94%|█████████▍| 79/84 [19:56<01:15, 15.12s/it] 95%|█████████▌| 80/84 [20:12<01:00, 15.14s/it] 96%|█████████▋| 81/84 [20:27<00:45, 15.14s/it] 98%|█████████▊| 82/84 [20:42<00:30, 15.15s/it] 99%|█████████▉| 83/84 [20:57<00:15, 15.15s/it]100%|██████████| 84/84 [21:02<00:00, 12.19s/it]100%|██████████| 84/84 [21:02<00:00, 15.04s/it]
Some weights of the model checkpoint at microsoft/deberta-xxlarge-v2 were not used when initializing DebertaV2ForMaskedLM: ['deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']
- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-xxlarge-v2 and are newly initialized: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Found cached dataset jigsaw_toxicity_pred (/home/danielezer/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=jigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85)
Saving hidden states
Running evaluate with the following arguments:
Run Number: 3Model: 'deberta'
Prompt Number: '0'
Number of Examples: '500'
Dataset Name: 'jigsaw_toxicity_pred'
Dataset Directory: 'jigsaw'
No Data Balance: 'True'
Toxicity Threshold: '0'
Logistic regression accuracy: 0.856
CCS accuracy: 0.628
Finished running generate and evaluate with all model configurations
Copying templates file templates/jigsaw-toxicity-pred-templates.yaml to /opt/conda/lib/python3.7/site-packages/promptsource/templates/jigsaw_toxicity_pred/templates.yaml
Successfully copied templates file
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Running generate with the following arguments:
Run Number: 4Model: 'deberta'
Prompt Number: '0'
Number of Examples: '500'
Dataset Name: 'jigsaw_toxicity_pred'
Dataset Directory: 'jigsaw'
No Data Balance: 'False'
Toxicity Threshold: '0'
Loading model
Loading dataloader
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 249.11it/s]
Loading cached processed dataset at /home/danielezer/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=jigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85/cache-c6571a49730c6301.arrow
Token indices sequence length is longer than the specified maximum sequence length for this model (514 > 512). Running this sequence through the model will result in indexing errors
Balancing Data
Generating hidden states
  0%|          | 0/84 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
  1%|          | 1/84 [00:15<21:10, 15.30s/it]  2%|▏         | 2/84 [00:30<20:48, 15.23s/it]  4%|▎         | 3/84 [00:45<20:32, 15.22s/it]  5%|▍         | 4/84 [01:00<20:14, 15.18s/it]  6%|▌         | 5/84 [01:15<19:58, 15.17s/it]  7%|▋         | 6/84 [01:31<19:41, 15.15s/it]  8%|▊         | 7/84 [01:46<19:26, 15.15s/it] 10%|▉         | 8/84 [02:01<19:11, 15.15s/it] 11%|█         | 9/84 [02:16<18:54, 15.12s/it] 12%|█▏        | 10/84 [02:31<18:40, 15.14s/it] 13%|█▎        | 11/84 [02:46<18:24, 15.13s/it] 14%|█▍        | 12/84 [03:01<18:08, 15.12s/it] 15%|█▌        | 13/84 [03:16<17:53, 15.12s/it] 17%|█▋        | 14/84 [03:32<17:39, 15.13s/it] 18%|█▊        | 15/84 [03:47<17:23, 15.12s/it] 19%|█▉        | 16/84 [04:02<17:07, 15.11s/it] 20%|██        | 17/84 [04:17<16:52, 15.11s/it] 21%|██▏       | 18/84 [04:32<16:37, 15.12s/it] 23%|██▎       | 19/84 [04:47<16:23, 15.13s/it] 24%|██▍       | 20/84 [05:02<16:08, 15.14s/it] 25%|██▌       | 21/84 [05:18<15:55, 15.17s/it] 26%|██▌       | 22/84 [05:33<15:39, 15.15s/it] 27%|██▋       | 23/84 [05:48<15:24, 15.16s/it] 29%|██▊       | 24/84 [06:03<15:09, 15.16s/it] 30%|██▉       | 25/84 [06:18<14:53, 15.14s/it] 31%|███       | 26/84 [06:33<14:38, 15.15s/it] 32%|███▏      | 27/84 [06:48<14:24, 15.16s/it] 33%|███▎      | 28/84 [07:04<14:08, 15.16s/it] 35%|███▍      | 29/84 [07:19<13:53, 15.16s/it] 36%|███▌      | 30/84 [07:34<13:37, 15.15s/it] 37%|███▋      | 31/84 [07:49<13:23, 15.17s/it] 38%|███▊      | 32/84 [08:04<13:07, 15.14s/it] 39%|███▉      | 33/84 [08:19<12:52, 15.15s/it] 40%|████      | 34/84 [08:35<12:38, 15.16s/it] 42%|████▏     | 35/84 [08:50<12:22, 15.16s/it] 43%|████▎     | 36/84 [09:05<12:07, 15.17s/it] 44%|████▍     | 37/84 [09:20<11:53, 15.18s/it] 45%|████▌     | 38/84 [09:35<11:37, 15.17s/it] 46%|████▋     | 39/84 [09:50<11:21, 15.15s/it] 48%|████▊     | 40/84 [10:05<11:06, 15.15s/it] 49%|████▉     | 41/84 [10:21<10:51, 15.14s/it] 50%|█████     | 42/84 [10:36<10:36, 15.15s/it] 51%|█████     | 43/84 [10:51<10:21, 15.15s/it] 52%|█████▏    | 44/84 [11:06<10:06, 15.16s/it] 54%|█████▎    | 45/84 [11:21<09:50, 15.14s/it] 55%|█████▍    | 46/84 [11:36<09:35, 15.13s/it] 56%|█████▌    | 47/84 [11:52<09:20, 15.15s/it] 57%|█████▋    | 48/84 [12:07<09:05, 15.15s/it] 58%|█████▊    | 49/84 [12:22<08:49, 15.14s/it] 60%|█████▉    | 50/84 [12:37<08:34, 15.14s/it] 61%|██████    | 51/84 [12:52<08:20, 15.17s/it] 62%|██████▏   | 52/84 [13:07<08:05, 15.16s/it] 63%|██████▎   | 53/84 [13:22<07:49, 15.16s/it] 64%|██████▍   | 54/84 [13:38<07:34, 15.14s/it] 65%|██████▌   | 55/84 [13:53<07:18, 15.14s/it] 67%|██████▋   | 56/84 [14:08<07:03, 15.13s/it] 68%|██████▊   | 57/84 [14:23<06:48, 15.13s/it] 69%|██████▉   | 58/84 [14:38<06:33, 15.14s/it] 70%|███████   | 59/84 [14:53<06:18, 15.15s/it] 71%|███████▏  | 60/84 [15:08<06:03, 15.14s/it] 73%|███████▎  | 61/84 [15:24<05:48, 15.14s/it] 74%|███████▍  | 62/84 [15:39<05:33, 15.14s/it] 75%|███████▌  | 63/84 [15:54<05:17, 15.14s/it] 76%|███████▌  | 64/84 [16:09<05:02, 15.14s/it] 77%|███████▋  | 65/84 [16:24<04:47, 15.14s/it] 79%|███████▊  | 66/84 [16:39<04:32, 15.13s/it] 80%|███████▉  | 67/84 [16:54<04:17, 15.15s/it] 81%|████████  | 68/84 [17:10<04:02, 15.15s/it] 82%|████████▏ | 69/84 [17:25<03:47, 15.14s/it] 83%|████████▎ | 70/84 [17:40<03:31, 15.13s/it] 85%|████████▍ | 71/84 [17:55<03:16, 15.13s/it] 86%|████████▌ | 72/84 [18:10<03:01, 15.13s/it] 87%|████████▋ | 73/84 [18:25<02:46, 15.13s/it] 88%|████████▊ | 74/84 [18:40<02:31, 15.14s/it] 89%|████████▉ | 75/84 [18:55<02:16, 15.13s/it] 90%|█████████ | 76/84 [19:11<02:01, 15.13s/it] 92%|█████████▏| 77/84 [19:26<01:45, 15.13s/it] 93%|█████████▎| 78/84 [19:41<01:30, 15.12s/it] 94%|█████████▍| 79/84 [19:56<01:15, 15.15s/it] 95%|█████████▌| 80/84 [20:11<01:00, 15.15s/it] 96%|█████████▋| 81/84 [20:26<00:45, 15.14s/it] 98%|█████████▊| 82/84 [20:41<00:30, 15.16s/it] 99%|█████████▉| 83/84 [20:57<00:15, 15.14s/it]100%|██████████| 84/84 [21:02<00:00, 12.18s/it]100%|██████████| 84/84 [21:02<00:00, 15.03s/it]
Some weights of the model checkpoint at microsoft/deberta-xxlarge-v2 were not used when initializing DebertaV2ForMaskedLM: ['deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']
- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-xxlarge-v2 and are newly initialized: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Found cached dataset jigsaw_toxicity_pred (/home/danielezer/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=jigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85)
Saving hidden states
Running evaluate with the following arguments:
Run Number: 4Model: 'deberta'
Prompt Number: '0'
Number of Examples: '500'
Dataset Name: 'jigsaw_toxicity_pred'
Dataset Directory: 'jigsaw'
No Data Balance: 'False'
Toxicity Threshold: '0'
Logistic regression accuracy: 0.772
CCS accuracy: 0.5760000000000001
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Running generate with the following arguments:
Run Number: 4Model: 'deberta'
Prompt Number: '0'
Number of Examples: '500'
Dataset Name: 'jigsaw_toxicity_pred'
Dataset Directory: 'jigsaw'
No Data Balance: 'True'
Toxicity Threshold: '0'
Loading model
Loading dataloader
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 241.51it/s]
Loading cached processed dataset at /home/danielezer/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=jigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85/cache-c6571a49730c6301.arrow
Token indices sequence length is longer than the specified maximum sequence length for this model (634 > 512). Running this sequence through the model will result in indexing errors
Generating hidden states
  0%|          | 0/84 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
  1%|          | 1/84 [00:15<21:05, 15.25s/it]  2%|▏         | 2/84 [00:30<20:47, 15.21s/it]  4%|▎         | 3/84 [00:45<20:30, 15.19s/it]  5%|▍         | 4/84 [01:00<20:14, 15.18s/it]  6%|▌         | 5/84 [01:15<19:57, 15.15s/it]  7%|▋         | 6/84 [01:31<19:42, 15.16s/it]  8%|▊         | 7/84 [01:46<19:27, 15.16s/it] 10%|▉         | 8/84 [02:01<19:11, 15.15s/it] 11%|█         | 9/84 [02:16<18:56, 15.15s/it] 12%|█▏        | 10/84 [02:31<18:41, 15.15s/it] 13%|█▎        | 11/84 [02:46<18:25, 15.15s/it] 14%|█▍        | 12/84 [03:01<18:11, 15.16s/it] 15%|█▌        | 13/84 [03:17<17:56, 15.16s/it] 17%|█▋        | 14/84 [03:32<17:40, 15.15s/it] 18%|█▊        | 15/84 [03:47<17:24, 15.14s/it] 19%|█▉        | 16/84 [04:02<17:09, 15.14s/it] 20%|██        | 17/84 [04:17<16:53, 15.12s/it] 21%|██▏       | 18/84 [04:32<16:38, 15.12s/it] 23%|██▎       | 19/84 [04:47<16:22, 15.12s/it] 24%|██▍       | 20/84 [05:02<16:08, 15.13s/it] 25%|██▌       | 21/84 [05:18<15:54, 15.14s/it] 26%|██▌       | 22/84 [05:33<15:39, 15.16s/it] 27%|██▋       | 23/84 [05:48<15:23, 15.14s/it] 29%|██▊       | 24/84 [06:03<15:08, 15.14s/it] 30%|██▉       | 25/84 [06:18<14:53, 15.15s/it] 31%|███       | 26/84 [06:33<14:38, 15.14s/it] 32%|███▏      | 27/84 [06:48<14:21, 15.12s/it] 33%|███▎      | 28/84 [07:04<14:07, 15.13s/it] 35%|███▍      | 29/84 [07:19<13:52, 15.13s/it] 36%|███▌      | 30/84 [07:34<13:37, 15.14s/it] 37%|███▋      | 31/84 [07:49<13:22, 15.15s/it] 38%|███▊      | 32/84 [08:04<13:06, 15.13s/it] 39%|███▉      | 33/84 [08:19<12:51, 15.13s/it] 40%|████      | 34/84 [08:34<12:36, 15.13s/it] 42%|████▏     | 35/84 [08:50<12:22, 15.14s/it] 43%|████▎     | 36/84 [09:05<12:06, 15.14s/it] 44%|████▍     | 37/84 [09:20<11:52, 15.15s/it] 45%|████▌     | 38/84 [09:35<11:37, 15.16s/it] 46%|████▋     | 39/84 [09:50<11:23, 15.18s/it] 48%|████▊     | 40/84 [10:05<11:06, 15.16s/it] 49%|████▉     | 41/84 [10:21<10:51, 15.15s/it] 50%|█████     | 42/84 [10:36<10:36, 15.15s/it] 51%|█████     | 43/84 [10:51<10:21, 15.16s/it] 52%|█████▏    | 44/84 [11:06<10:06, 15.15s/it] 54%|█████▎    | 45/84 [11:21<09:51, 15.16s/it] 55%|█████▍    | 46/84 [11:36<09:35, 15.15s/it] 56%|█████▌    | 47/84 [11:51<09:20, 15.15s/it] 57%|█████▋    | 48/84 [12:07<09:05, 15.14s/it] 58%|█████▊    | 49/84 [12:22<08:49, 15.13s/it] 60%|█████▉    | 50/84 [12:37<08:34, 15.12s/it] 61%|██████    | 51/84 [12:52<08:19, 15.12s/it] 62%|██████▏   | 52/84 [13:07<08:04, 15.14s/it] 63%|██████▎   | 53/84 [13:22<07:49, 15.14s/it] 64%|██████▍   | 54/84 [13:37<07:34, 15.14s/it] 65%|██████▌   | 55/84 [13:52<07:19, 15.14s/it] 67%|██████▋   | 56/84 [14:08<07:03, 15.14s/it] 68%|██████▊   | 57/84 [14:23<06:48, 15.14s/it] 69%|██████▉   | 58/84 [14:38<06:33, 15.13s/it] 70%|███████   | 59/84 [14:53<06:18, 15.15s/it] 71%|███████▏  | 60/84 [15:08<06:03, 15.15s/it] 73%|███████▎  | 61/84 [15:23<05:48, 15.14s/it] 74%|███████▍  | 62/84 [15:38<05:33, 15.14s/it] 75%|███████▌  | 63/84 [15:54<05:17, 15.13s/it] 76%|███████▌  | 64/84 [16:09<05:02, 15.12s/it] 77%|███████▋  | 65/84 [16:24<04:47, 15.15s/it] 79%|███████▊  | 66/84 [16:39<04:32, 15.15s/it] 80%|███████▉  | 67/84 [16:54<04:17, 15.16s/it] 81%|████████  | 68/84 [17:09<04:02, 15.14s/it] 82%|████████▏ | 69/84 [17:24<03:47, 15.14s/it] 83%|████████▎ | 70/84 [17:40<03:31, 15.14s/it] 85%|████████▍ | 71/84 [17:55<03:16, 15.14s/it] 86%|████████▌ | 72/84 [18:10<03:01, 15.13s/it] 87%|████████▋ | 73/84 [18:25<02:46, 15.12s/it] 88%|████████▊ | 74/84 [18:40<02:31, 15.10s/it] 89%|████████▉ | 75/84 [18:55<02:16, 15.12s/it] 90%|█████████ | 76/84 [19:10<02:00, 15.12s/it] 92%|█████████▏| 77/84 [19:25<01:45, 15.11s/it] 93%|█████████▎| 78/84 [19:40<01:30, 15.10s/it] 94%|█████████▍| 79/84 [19:56<01:15, 15.11s/it] 95%|█████████▌| 80/84 [20:11<01:00, 15.13s/it] 96%|█████████▋| 81/84 [20:26<00:45, 15.13s/it] 98%|█████████▊| 82/84 [20:41<00:30, 15.13s/it] 99%|█████████▉| 83/84 [20:56<00:15, 15.15s/it]100%|██████████| 84/84 [21:02<00:00, 12.19s/it]100%|██████████| 84/84 [21:02<00:00, 15.02s/it]
Some weights of the model checkpoint at microsoft/deberta-xxlarge-v2 were not used when initializing DebertaV2ForMaskedLM: ['deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']
- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-xxlarge-v2 and are newly initialized: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Found cached dataset jigsaw_toxicity_pred (/home/danielezer/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=jigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85)
Saving hidden states
Running evaluate with the following arguments:
Run Number: 4Model: 'deberta'
Prompt Number: '0'
Number of Examples: '500'
Dataset Name: 'jigsaw_toxicity_pred'
Dataset Directory: 'jigsaw'
No Data Balance: 'True'
Toxicity Threshold: '0'
Logistic regression accuracy: 0.896
CCS accuracy: 0.588
Finished running generate and evaluate with all model configurations
Copying templates file templates/jigsaw-toxicity-pred-templates.yaml to /opt/conda/lib/python3.7/site-packages/promptsource/templates/jigsaw_toxicity_pred/templates.yaml
Successfully copied templates file
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Running generate with the following arguments:
Run Number: 5Model: 'deberta'
Prompt Number: '0'
Number of Examples: '500'
Dataset Name: 'jigsaw_toxicity_pred'
Dataset Directory: 'jigsaw'
No Data Balance: 'False'
Toxicity Threshold: '0'
Loading model
Loading dataloader
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 241.20it/s]
Loading cached processed dataset at /home/danielezer/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=jigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85/cache-c6571a49730c6301.arrow
Token indices sequence length is longer than the specified maximum sequence length for this model (557 > 512). Running this sequence through the model will result in indexing errors
Balancing Data
Generating hidden states
  0%|          | 0/84 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
  1%|          | 1/84 [00:15<21:08, 15.28s/it]  2%|▏         | 2/84 [00:30<20:49, 15.24s/it]  4%|▎         | 3/84 [00:45<20:31, 15.20s/it]  5%|▍         | 4/84 [01:00<20:15, 15.20s/it]  6%|▌         | 5/84 [01:15<19:58, 15.17s/it]  7%|▋         | 6/84 [01:31<19:41, 15.15s/it]  8%|▊         | 7/84 [01:46<19:26, 15.15s/it] 10%|▉         | 8/84 [02:01<19:11, 15.15s/it] 11%|█         | 9/84 [02:16<18:55, 15.15s/it] 12%|█▏        | 10/84 [02:31<18:38, 15.12s/it] 13%|█▎        | 11/84 [02:46<18:24, 15.13s/it] 14%|█▍        | 12/84 [03:01<18:09, 15.13s/it] 15%|█▌        | 13/84 [03:17<17:54, 15.14s/it] 17%|█▋        | 14/84 [03:32<17:39, 15.14s/it] 18%|█▊        | 15/84 [03:47<17:24, 15.14s/it] 19%|█▉        | 16/84 [04:02<17:09, 15.14s/it] 20%|██        | 17/84 [04:17<16:53, 15.13s/it] 21%|██▏       | 18/84 [04:32<16:38, 15.13s/it] 23%|██▎       | 19/84 [04:47<16:22, 15.12s/it] 24%|██▍       | 20/84 [05:03<16:10, 15.16s/it] 25%|██▌       | 21/84 [05:18<15:54, 15.16s/it] 26%|██▌       | 22/84 [05:33<15:38, 15.14s/it] 27%|██▋       | 23/84 [05:48<15:23, 15.14s/it] 29%|██▊       | 24/84 [06:03<15:08, 15.14s/it] 30%|██▉       | 25/84 [06:18<14:52, 15.13s/it] 31%|███       | 26/84 [06:33<14:37, 15.12s/it] 32%|███▏      | 27/84 [06:48<14:22, 15.13s/it] 33%|███▎      | 28/84 [07:04<14:06, 15.12s/it] 35%|███▍      | 29/84 [07:19<13:51, 15.12s/it] 36%|███▌      | 30/84 [07:34<13:36, 15.11s/it] 37%|███▋      | 31/84 [07:49<13:20, 15.11s/it] 38%|███▊      | 32/84 [08:04<13:05, 15.11s/it] 39%|███▉      | 33/84 [08:19<12:51, 15.13s/it] 40%|████      | 34/84 [08:34<12:36, 15.13s/it] 42%|████▏     | 35/84 [08:49<12:21, 15.13s/it] 43%|████▎     | 36/84 [09:04<12:05, 15.12s/it] 44%|████▍     | 37/84 [09:20<11:50, 15.13s/it] 45%|████▌     | 38/84 [09:35<11:36, 15.15s/it] 46%|████▋     | 39/84 [09:50<11:20, 15.13s/it] 48%|████▊     | 40/84 [10:05<11:05, 15.11s/it] 49%|████▉     | 41/84 [10:20<10:50, 15.13s/it] 50%|█████     | 42/84 [10:35<10:36, 15.16s/it] 51%|█████     | 43/84 [10:50<10:20, 15.14s/it] 52%|█████▏    | 44/84 [11:06<10:05, 15.15s/it] 54%|█████▎    | 45/84 [11:21<09:50, 15.13s/it] 55%|█████▍    | 46/84 [11:36<09:34, 15.13s/it] 56%|█████▌    | 47/84 [11:51<09:19, 15.12s/it] 57%|█████▋    | 48/84 [12:06<09:04, 15.12s/it] 58%|█████▊    | 49/84 [12:21<08:48, 15.10s/it] 60%|█████▉    | 50/84 [12:36<08:33, 15.10s/it] 61%|██████    | 51/84 [12:51<08:18, 15.12s/it] 62%|██████▏   | 52/84 [13:07<08:04, 15.13s/it] 63%|██████▎   | 53/84 [13:22<07:49, 15.14s/it] 64%|██████▍   | 54/84 [13:37<07:34, 15.14s/it] 65%|██████▌   | 55/84 [13:52<07:18, 15.13s/it] 67%|██████▋   | 56/84 [14:07<07:03, 15.14s/it] 68%|██████▊   | 57/84 [14:22<06:48, 15.13s/it] 69%|██████▉   | 58/84 [14:37<06:33, 15.12s/it] 70%|███████   | 59/84 [14:52<06:17, 15.11s/it] 71%|███████▏  | 60/84 [15:08<06:02, 15.11s/it] 73%|███████▎  | 61/84 [15:23<05:47, 15.12s/it] 74%|███████▍  | 62/84 [15:38<05:32, 15.13s/it] 75%|███████▌  | 63/84 [15:53<05:17, 15.11s/it] 76%|███████▌  | 64/84 [16:08<05:02, 15.11s/it] 77%|███████▋  | 65/84 [16:23<04:47, 15.13s/it] 79%|███████▊  | 66/84 [16:38<04:32, 15.12s/it] 80%|███████▉  | 67/84 [16:53<04:17, 15.13s/it] 81%|████████  | 68/84 [17:09<04:02, 15.13s/it] 82%|████████▏ | 69/84 [17:24<03:46, 15.13s/it] 83%|████████▎ | 70/84 [17:39<03:31, 15.13s/it] 85%|████████▍ | 71/84 [17:54<03:16, 15.12s/it] 86%|████████▌ | 72/84 [18:09<03:01, 15.14s/it] 87%|████████▋ | 73/84 [18:24<02:46, 15.15s/it] 88%|████████▊ | 74/84 [18:39<02:31, 15.13s/it] 89%|████████▉ | 75/84 [18:55<02:16, 15.13s/it] 90%|█████████ | 76/84 [19:10<02:01, 15.13s/it] 92%|█████████▏| 77/84 [19:25<01:46, 15.15s/it] 93%|█████████▎| 78/84 [19:40<01:30, 15.13s/it] 94%|█████████▍| 79/84 [19:55<01:15, 15.12s/it] 95%|█████████▌| 80/84 [20:10<01:00, 15.12s/it] 96%|█████████▋| 81/84 [20:25<00:45, 15.11s/it] 98%|█████████▊| 82/84 [20:40<00:30, 15.12s/it] 99%|█████████▉| 83/84 [20:56<00:15, 15.12s/it]100%|██████████| 84/84 [21:01<00:00, 12.15s/it]100%|██████████| 84/84 [21:01<00:00, 15.02s/it]
/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,
Some weights of the model checkpoint at microsoft/deberta-xxlarge-v2 were not used when initializing DebertaV2ForMaskedLM: ['deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']
- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-xxlarge-v2 and are newly initialized: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Found cached dataset jigsaw_toxicity_pred (/home/danielezer/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=jigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85)
Saving hidden states
Running evaluate with the following arguments:
Run Number: 5Model: 'deberta'
Prompt Number: '0'
Number of Examples: '500'
Dataset Name: 'jigsaw_toxicity_pred'
Dataset Directory: 'jigsaw'
No Data Balance: 'False'
Toxicity Threshold: '0'
Logistic regression accuracy: 0.748
CCS accuracy: 0.612
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Running generate with the following arguments:
Run Number: 5Model: 'deberta'
Prompt Number: '0'
Number of Examples: '500'
Dataset Name: 'jigsaw_toxicity_pred'
Dataset Directory: 'jigsaw'
No Data Balance: 'True'
Toxicity Threshold: '0'
Loading model
Loading dataloader
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 249.33it/s]
Loading cached processed dataset at /home/danielezer/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=jigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85/cache-c6571a49730c6301.arrow
Token indices sequence length is longer than the specified maximum sequence length for this model (658 > 512). Running this sequence through the model will result in indexing errors
Generating hidden states
  0%|          | 0/84 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
  1%|          | 1/84 [00:15<21:16, 15.38s/it]  2%|▏         | 2/84 [00:30<20:50, 15.25s/it]  4%|▎         | 3/84 [00:45<20:31, 15.20s/it]  5%|▍         | 4/84 [01:00<20:13, 15.17s/it]  6%|▌         | 5/84 [01:16<20:01, 15.21s/it]  7%|▋         | 6/84 [01:31<19:43, 15.18s/it]  8%|▊         | 7/84 [01:46<19:27, 15.16s/it] 10%|▉         | 8/84 [02:01<19:13, 15.18s/it] 11%|█         | 9/84 [02:16<18:57, 15.17s/it] 12%|█▏        | 10/84 [02:31<18:42, 15.18s/it] 13%|█▎        | 11/84 [02:47<18:27, 15.17s/it] 14%|█▍        | 12/84 [03:02<18:11, 15.16s/it] 15%|█▌        | 13/84 [03:17<17:56, 15.16s/it] 17%|█▋        | 14/84 [03:32<17:40, 15.15s/it] 18%|█▊        | 15/84 [03:47<17:25, 15.15s/it] 19%|█▉        | 16/84 [04:02<17:09, 15.14s/it] 20%|██        | 17/84 [04:17<16:54, 15.14s/it] 21%|██▏       | 18/84 [04:32<16:38, 15.13s/it] 23%|██▎       | 19/84 [04:48<16:22, 15.12s/it] 24%|██▍       | 20/84 [05:03<16:09, 15.14s/it] 25%|██▌       | 21/84 [05:18<15:54, 15.15s/it] 26%|██▌       | 22/84 [05:33<15:38, 15.14s/it] 27%|██▋       | 23/84 [05:48<15:24, 15.15s/it] 29%|██▊       | 24/84 [06:03<15:08, 15.15s/it] 30%|██▉       | 25/84 [06:19<14:53, 15.15s/it] 31%|███       | 26/84 [06:34<14:38, 15.14s/it] 32%|███▏      | 27/84 [06:49<14:23, 15.14s/it] 33%|███▎      | 28/84 [07:04<14:07, 15.14s/it] 35%|███▍      | 29/84 [07:19<13:52, 15.14s/it] 36%|███▌      | 30/84 [07:34<13:38, 15.16s/it] 37%|███▋      | 31/84 [07:49<13:22, 15.15s/it] 38%|███▊      | 32/84 [08:05<13:08, 15.16s/it] 39%|███▉      | 33/84 [08:20<12:52, 15.15s/it] 40%|████      | 34/84 [08:35<12:36, 15.14s/it] 42%|████▏     | 35/84 [08:50<12:22, 15.16s/it] 43%|████▎     | 36/84 [09:05<12:07, 15.15s/it] 44%|████▍     | 37/84 [09:20<11:51, 15.14s/it] 45%|████▌     | 38/84 [09:35<11:36, 15.13s/it] 46%|████▋     | 39/84 [09:51<11:20, 15.13s/it] 48%|████▊     | 40/84 [10:06<11:05, 15.13s/it] 49%|████▉     | 41/84 [10:21<10:50, 15.14s/it] 50%|█████     | 42/84 [10:36<10:36, 15.16s/it] 51%|█████     | 43/84 [10:51<10:21, 15.16s/it] 52%|█████▏    | 44/84 [11:06<10:05, 15.15s/it] 54%|█████▎    | 45/84 [11:21<09:51, 15.15s/it] 55%|█████▍    | 46/84 [11:37<09:35, 15.15s/it] 56%|█████▌    | 47/84 [11:52<09:20, 15.14s/it] 57%|█████▋    | 48/84 [12:07<09:04, 15.13s/it] 58%|█████▊    | 49/84 [12:22<08:49, 15.14s/it] 60%|█████▉    | 50/84 [12:37<08:35, 15.16s/it] 61%|██████    | 51/84 [12:52<08:20, 15.15s/it] 62%|██████▏   | 52/84 [13:08<08:05, 15.16s/it] 63%|██████▎   | 53/84 [13:23<07:51, 15.19s/it] 64%|██████▍   | 54/84 [13:38<07:35, 15.19s/it] 65%|██████▌   | 55/84 [13:53<07:19, 15.17s/it] 67%|██████▋   | 56/84 [14:08<07:04, 15.15s/it] 68%|██████▊   | 57/84 [14:23<06:48, 15.15s/it] 69%|██████▉   | 58/84 [14:38<06:33, 15.14s/it] 70%|███████   | 59/84 [14:54<06:18, 15.14s/it] 71%|███████▏  | 60/84 [15:09<06:03, 15.15s/it] 73%|███████▎  | 61/84 [15:24<05:48, 15.15s/it] 74%|███████▍  | 62/84 [15:39<05:33, 15.14s/it] 75%|███████▌  | 63/84 [15:54<05:17, 15.13s/it] 76%|███████▌  | 64/84 [16:09<05:03, 15.15s/it] 77%|███████▋  | 65/84 [16:24<04:47, 15.14s/it] 79%|███████▊  | 66/84 [16:40<04:32, 15.16s/it] 80%|███████▉  | 67/84 [16:55<04:17, 15.15s/it] 81%|████████  | 68/84 [17:10<04:02, 15.14s/it] 82%|████████▏ | 69/84 [17:25<03:47, 15.15s/it] 83%|████████▎ | 70/84 [17:40<03:32, 15.16s/it] 85%|████████▍ | 71/84 [17:55<03:16, 15.14s/it] 86%|████████▌ | 72/84 [18:10<03:01, 15.12s/it] 87%|████████▋ | 73/84 [18:25<02:46, 15.10s/it] 88%|████████▊ | 74/84 [18:41<02:30, 15.09s/it] 89%|████████▉ | 75/84 [18:56<02:15, 15.10s/it] 90%|█████████ | 76/84 [19:11<02:00, 15.11s/it] 92%|█████████▏| 77/84 [19:26<01:45, 15.12s/it] 93%|█████████▎| 78/84 [19:41<01:30, 15.11s/it] 94%|█████████▍| 79/84 [19:56<01:15, 15.11s/it] 95%|█████████▌| 80/84 [20:11<01:00, 15.13s/it] 96%|█████████▋| 81/84 [20:26<00:45, 15.14s/it] 98%|█████████▊| 82/84 [20:42<00:30, 15.14s/it] 99%|█████████▉| 83/84 [20:57<00:15, 15.13s/it]100%|██████████| 84/84 [21:02<00:00, 12.17s/it]100%|██████████| 84/84 [21:02<00:00, 15.03s/it]
Some weights of the model checkpoint at microsoft/deberta-xxlarge-v2 were not used when initializing DebertaV2ForMaskedLM: ['deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']
- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-xxlarge-v2 and are newly initialized: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Found cached dataset jigsaw_toxicity_pred (/home/danielezer/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=jigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85)
Saving hidden states
Running evaluate with the following arguments:
Run Number: 5Model: 'deberta'
Prompt Number: '0'
Number of Examples: '500'
Dataset Name: 'jigsaw_toxicity_pred'
Dataset Directory: 'jigsaw'
No Data Balance: 'True'
Toxicity Threshold: '0'
Logistic regression accuracy: 0.88
CCS accuracy: 0.528
Finished running generate and evaluate with all model configurations
Copying templates file templates/jigsaw-toxicity-pred-templates.yaml to /opt/conda/lib/python3.7/site-packages/promptsource/templates/jigsaw_toxicity_pred/templates.yaml
Successfully copied templates file
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Running generate with the following arguments:
Run Number: 6Model: 'deberta'
Prompt Number: '0'
Number of Examples: '500'
Dataset Name: 'jigsaw_toxicity_pred'
Dataset Directory: 'jigsaw'
No Data Balance: 'False'
Toxicity Threshold: '0'
Loading model
Loading dataloader
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 242.55it/s]
Loading cached processed dataset at /home/danielezer/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=jigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85/cache-c6571a49730c6301.arrow
Token indices sequence length is longer than the specified maximum sequence length for this model (555 > 512). Running this sequence through the model will result in indexing errors
Balancing Data
Generating hidden states
  0%|          | 0/84 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
  1%|          | 1/84 [00:15<21:07, 15.27s/it]  2%|▏         | 2/84 [00:30<20:46, 15.21s/it]  4%|▎         | 3/84 [00:45<20:30, 15.20s/it]  5%|▍         | 4/84 [01:00<20:14, 15.18s/it]  6%|▌         | 5/84 [01:15<19:58, 15.18s/it]  7%|▋         | 6/84 [01:31<19:41, 15.15s/it]  8%|▊         | 7/84 [01:46<19:26, 15.15s/it] 10%|▉         | 8/84 [02:01<19:10, 15.14s/it] 11%|█         | 9/84 [02:16<18:56, 15.15s/it] 12%|█▏        | 10/84 [02:31<18:41, 15.15s/it] 13%|█▎        | 11/84 [02:46<18:23, 15.12s/it] 14%|█▍        | 12/84 [03:01<18:09, 15.13s/it] 15%|█▌        | 13/84 [03:16<17:53, 15.12s/it] 17%|█▋        | 14/84 [03:32<17:38, 15.12s/it] 18%|█▊        | 15/84 [03:47<17:23, 15.12s/it] 19%|█▉        | 16/84 [04:02<17:08, 15.13s/it] 20%|██        | 17/84 [04:17<16:54, 15.14s/it] 21%|██▏       | 18/84 [04:32<16:40, 15.15s/it] 23%|██▎       | 19/84 [04:47<16:26, 15.18s/it] 24%|██▍       | 20/84 [05:03<16:10, 15.16s/it] 25%|██▌       | 21/84 [05:18<15:54, 15.15s/it] 26%|██▌       | 22/84 [05:33<15:38, 15.14s/it] 27%|██▋       | 23/84 [05:48<15:24, 15.16s/it] 29%|██▊       | 24/84 [06:03<15:09, 15.16s/it] 30%|██▉       | 25/84 [06:18<14:53, 15.15s/it] 31%|███       | 26/84 [06:33<14:37, 15.13s/it] 32%|███▏      | 27/84 [06:49<14:23, 15.15s/it] 33%|███▎      | 28/84 [07:04<14:07, 15.14s/it] 35%|███▍      | 29/84 [07:19<13:54, 15.17s/it] 36%|███▌      | 30/84 [07:34<13:38, 15.15s/it] 37%|███▋      | 31/84 [07:49<13:24, 15.17s/it] 38%|███▊      | 32/84 [08:04<13:08, 15.17s/it] 39%|███▉      | 33/84 [08:19<12:52, 15.14s/it] 40%|████      | 34/84 [08:35<12:36, 15.13s/it] 42%|████▏     | 35/84 [08:50<12:21, 15.13s/it] 43%|████▎     | 36/84 [09:05<12:05, 15.12s/it] 44%|████▍     | 37/84 [09:20<11:50, 15.13s/it] 45%|████▌     | 38/84 [09:35<11:35, 15.12s/it] 46%|████▋     | 39/84 [09:50<11:20, 15.12s/it] 48%|████▊     | 40/84 [10:05<11:05, 15.13s/it] 49%|████▉     | 41/84 [10:20<10:49, 15.11s/it] 50%|█████     | 42/84 [10:35<10:34, 15.12s/it] 51%|█████     | 43/84 [10:51<10:19, 15.11s/it] 52%|█████▏    | 44/84 [11:06<10:04, 15.12s/it] 54%|█████▎    | 45/84 [11:21<09:50, 15.13s/it] 55%|█████▍    | 46/84 [11:36<09:35, 15.14s/it] 56%|█████▌    | 47/84 [11:51<09:19, 15.13s/it] 57%|█████▋    | 48/84 [12:06<09:04, 15.12s/it] 58%|█████▊    | 49/84 [12:21<08:48, 15.11s/it] 60%|█████▉    | 50/84 [12:36<08:33, 15.11s/it] 61%|██████    | 51/84 [12:52<08:18, 15.11s/it] 62%|██████▏   | 52/84 [13:07<08:03, 15.11s/it] 63%|██████▎   | 53/84 [13:22<07:48, 15.11s/it] 64%|██████▍   | 54/84 [13:37<07:33, 15.11s/it] 65%|██████▌   | 55/84 [13:52<07:18, 15.13s/it] 67%|██████▋   | 56/84 [14:07<07:03, 15.12s/it] 68%|██████▊   | 57/84 [14:22<06:48, 15.13s/it] 69%|██████▉   | 58/84 [14:38<06:34, 15.16s/it] 70%|███████   | 59/84 [14:53<06:18, 15.14s/it] 71%|███████▏  | 60/84 [15:08<06:03, 15.14s/it] 73%|███████▎  | 61/84 [15:23<05:48, 15.14s/it] 74%|███████▍  | 62/84 [15:38<05:32, 15.13s/it] 75%|███████▌  | 63/84 [15:53<05:17, 15.13s/it] 76%|███████▌  | 64/84 [16:08<05:02, 15.13s/it] 77%|███████▋  | 65/84 [16:23<04:47, 15.12s/it] 79%|███████▊  | 66/84 [16:39<04:32, 15.12s/it] 80%|███████▉  | 67/84 [16:54<04:16, 15.10s/it] 81%|████████  | 68/84 [17:09<04:01, 15.10s/it] 82%|████████▏ | 69/84 [17:24<03:46, 15.11s/it] 83%|████████▎ | 70/84 [17:39<03:31, 15.11s/it] 85%|████████▍ | 71/84 [17:54<03:16, 15.11s/it] 86%|████████▌ | 72/84 [18:09<03:01, 15.11s/it] 87%|████████▋ | 73/84 [18:24<02:46, 15.11s/it] 88%|████████▊ | 74/84 [18:39<02:31, 15.11s/it] 89%|████████▉ | 75/84 [18:54<02:16, 15.11s/it] 90%|█████████ | 76/84 [19:10<02:00, 15.12s/it] 92%|█████████▏| 77/84 [19:25<01:45, 15.13s/it] 93%|█████████▎| 78/84 [19:40<01:30, 15.12s/it] 94%|█████████▍| 79/84 [19:55<01:15, 15.12s/it] 95%|█████████▌| 80/84 [20:10<01:00, 15.12s/it] 96%|█████████▋| 81/84 [20:25<00:45, 15.11s/it] 98%|█████████▊| 82/84 [20:40<00:30, 15.11s/it] 99%|█████████▉| 83/84 [20:55<00:15, 15.11s/it]100%|██████████| 84/84 [21:01<00:00, 12.16s/it]100%|██████████| 84/84 [21:01<00:00, 15.01s/it]
Some weights of the model checkpoint at microsoft/deberta-xxlarge-v2 were not used when initializing DebertaV2ForMaskedLM: ['deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']
- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-xxlarge-v2 and are newly initialized: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Found cached dataset jigsaw_toxicity_pred (/home/danielezer/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=jigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85)
Saving hidden states
Running evaluate with the following arguments:
Run Number: 6Model: 'deberta'
Prompt Number: '0'
Number of Examples: '500'
Dataset Name: 'jigsaw_toxicity_pred'
Dataset Directory: 'jigsaw'
No Data Balance: 'False'
Toxicity Threshold: '0'
Logistic regression accuracy: 0.808
CCS accuracy: 0.608
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Running generate with the following arguments:
Run Number: 6Model: 'deberta'
Prompt Number: '0'
Number of Examples: '500'
Dataset Name: 'jigsaw_toxicity_pred'
Dataset Directory: 'jigsaw'
No Data Balance: 'True'
Toxicity Threshold: '0'
Loading model
Loading dataloader
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 248.28it/s]
Loading cached processed dataset at /home/danielezer/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=jigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85/cache-c6571a49730c6301.arrow
Token indices sequence length is longer than the specified maximum sequence length for this model (658 > 512). Running this sequence through the model will result in indexing errors
Generating hidden states
  0%|          | 0/84 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
  1%|          | 1/84 [00:15<21:08, 15.28s/it]  2%|▏         | 2/84 [00:30<20:48, 15.22s/it]  4%|▎         | 3/84 [00:45<20:32, 15.22s/it]  5%|▍         | 4/84 [01:00<20:14, 15.18s/it]  6%|▌         | 5/84 [01:15<19:58, 15.17s/it]  7%|▋         | 6/84 [01:31<19:43, 15.18s/it]  8%|▊         | 7/84 [01:46<19:27, 15.16s/it] 10%|▉         | 8/84 [02:01<19:13, 15.18s/it] 11%|█         | 9/84 [02:16<18:59, 15.19s/it] 12%|█▏        | 10/84 [02:31<18:42, 15.17s/it] 13%|█▎        | 11/84 [02:46<18:27, 15.17s/it] 14%|█▍        | 12/84 [03:02<18:10, 15.14s/it] 15%|█▌        | 13/84 [03:17<17:54, 15.13s/it] 17%|█▋        | 14/84 [03:32<17:40, 15.15s/it] 18%|█▊        | 15/84 [03:47<17:24, 15.14s/it] 19%|█▉        | 16/84 [04:02<17:09, 15.14s/it] 20%|██        | 17/84 [04:17<16:54, 15.14s/it] 21%|██▏       | 18/84 [04:32<16:39, 15.14s/it] 23%|██▎       | 19/84 [04:48<16:24, 15.15s/it] 24%|██▍       | 20/84 [05:03<16:08, 15.14s/it] 25%|██▌       | 21/84 [05:18<15:54, 15.15s/it] 26%|██▌       | 22/84 [05:33<15:39, 15.15s/it] 27%|██▋       | 23/84 [05:48<15:23, 15.14s/it] 29%|██▊       | 24/84 [06:03<15:09, 15.16s/it] 30%|██▉       | 25/84 [06:19<14:54, 15.16s/it] 31%|███       | 26/84 [06:34<14:37, 15.13s/it] 32%|███▏      | 27/84 [06:49<14:24, 15.16s/it] 33%|███▎      | 28/84 [07:04<14:08, 15.15s/it] 35%|███▍      | 29/84 [07:19<13:53, 15.16s/it] 36%|███▌      | 30/84 [07:34<13:37, 15.15s/it] 37%|███▋      | 31/84 [07:49<13:22, 15.14s/it] 38%|███▊      | 32/84 [08:05<13:08, 15.16s/it] 39%|███▉      | 33/84 [08:20<12:52, 15.15s/it] 40%|████      | 34/84 [08:35<12:37, 15.14s/it] 42%|████▏     | 35/84 [08:50<12:22, 15.16s/it] 43%|████▎     | 36/84 [09:05<12:08, 15.17s/it] 44%|████▍     | 37/84 [09:20<11:52, 15.16s/it] 45%|████▌     | 38/84 [09:35<11:36, 15.14s/it] 46%|████▋     | 39/84 [09:51<11:22, 15.16s/it] 48%|████▊     | 40/84 [10:06<11:06, 15.16s/it] 49%|████▉     | 41/84 [10:21<10:52, 15.17s/it] 50%|█████     | 42/84 [10:36<10:36, 15.16s/it] 51%|█████     | 43/84 [10:51<10:21, 15.16s/it] 52%|█████▏    | 44/84 [11:06<10:05, 15.15s/it] 54%|█████▎    | 45/84 [11:22<09:51, 15.17s/it] 55%|█████▍    | 46/84 [11:37<09:35, 15.15s/it] 56%|█████▌    | 47/84 [11:52<09:20, 15.15s/it] 57%|█████▋    | 48/84 [12:07<09:05, 15.14s/it] 58%|█████▊    | 49/84 [12:22<08:50, 15.15s/it] 60%|█████▉    | 50/84 [12:37<08:35, 15.16s/it] 61%|██████    | 51/84 [12:53<08:20, 15.16s/it] 62%|██████▏   | 52/84 [13:08<08:04, 15.15s/it] 63%|██████▎   | 53/84 [13:23<07:50, 15.16s/it] 64%|██████▍   | 54/84 [13:38<07:34, 15.15s/it] 65%|██████▌   | 55/84 [13:53<07:19, 15.14s/it] 67%|██████▋   | 56/84 [14:08<07:04, 15.16s/it] 68%|██████▊   | 57/84 [14:23<06:49, 15.16s/it] 69%|██████▉   | 58/84 [14:39<06:34, 15.16s/it] 70%|███████   | 59/84 [14:54<06:19, 15.18s/it] 71%|███████▏  | 60/84 [15:09<06:03, 15.17s/it] 73%|███████▎  | 61/84 [15:24<05:48, 15.17s/it] 74%|███████▍  | 62/84 [15:39<05:33, 15.16s/it] 75%|███████▌  | 63/84 [15:54<05:18, 15.14s/it] 76%|███████▌  | 64/84 [16:09<05:02, 15.13s/it] 77%|███████▋  | 65/84 [16:25<04:47, 15.13s/it] 79%|███████▊  | 66/84 [16:40<04:32, 15.12s/it] 80%|███████▉  | 67/84 [16:55<04:16, 15.12s/it] 81%|████████  | 68/84 [17:10<04:01, 15.12s/it] 82%|████████▏ | 69/84 [17:25<03:46, 15.13s/it] 83%|████████▎ | 70/84 [17:40<03:31, 15.14s/it] 85%|████████▍ | 71/84 [17:55<03:16, 15.14s/it] 86%|████████▌ | 72/84 [18:11<03:01, 15.14s/it] 87%|████████▋ | 73/84 [18:26<02:46, 15.16s/it] 88%|████████▊ | 74/84 [18:41<02:31, 15.14s/it] 89%|████████▉ | 75/84 [18:56<02:16, 15.14s/it] 90%|█████████ | 76/84 [19:11<02:01, 15.15s/it] 92%|█████████▏| 77/84 [19:26<01:45, 15.13s/it] 93%|█████████▎| 78/84 [19:41<01:30, 15.14s/it] 94%|█████████▍| 79/84 [19:57<01:15, 15.15s/it] 95%|█████████▌| 80/84 [20:12<01:00, 15.14s/it] 96%|█████████▋| 81/84 [20:27<00:45, 15.14s/it] 98%|█████████▊| 82/84 [20:42<00:30, 15.13s/it] 99%|█████████▉| 83/84 [20:57<00:15, 15.14s/it]100%|██████████| 84/84 [21:02<00:00, 12.19s/it]100%|██████████| 84/84 [21:02<00:00, 15.04s/it]
Some weights of the model checkpoint at microsoft/deberta-xxlarge-v2 were not used when initializing DebertaV2ForMaskedLM: ['deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']
- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-xxlarge-v2 and are newly initialized: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Found cached dataset jigsaw_toxicity_pred (/home/danielezer/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=jigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85)
Saving hidden states
Running evaluate with the following arguments:
Run Number: 6Model: 'deberta'
Prompt Number: '0'
Number of Examples: '500'
Dataset Name: 'jigsaw_toxicity_pred'
Dataset Directory: 'jigsaw'
No Data Balance: 'True'
Toxicity Threshold: '0'
Logistic regression accuracy: 0.892
CCS accuracy: 0.5640000000000001
Finished running generate and evaluate with all model configurations
Copying templates file templates/jigsaw-toxicity-pred-templates.yaml to /opt/conda/lib/python3.7/site-packages/promptsource/templates/jigsaw_toxicity_pred/templates.yaml
Successfully copied templates file
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Running generate with the following arguments:
Run Number: 7Model: 'deberta'
Prompt Number: '0'
Number of Examples: '500'
Dataset Name: 'jigsaw_toxicity_pred'
Dataset Directory: 'jigsaw'
No Data Balance: 'False'
Toxicity Threshold: '0'
Loading model
Loading dataloader
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 233.44it/s]
Loading cached processed dataset at /home/danielezer/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=jigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85/cache-c6571a49730c6301.arrow
Token indices sequence length is longer than the specified maximum sequence length for this model (667 > 512). Running this sequence through the model will result in indexing errors
Balancing Data
Generating hidden states
  0%|          | 0/84 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
  1%|          | 1/84 [00:15<21:12, 15.33s/it]  2%|▏         | 2/84 [00:30<20:49, 15.24s/it]  4%|▎         | 3/84 [00:45<20:32, 15.21s/it]  5%|▍         | 4/84 [01:00<20:15, 15.19s/it]  6%|▌         | 5/84 [01:15<19:58, 15.17s/it]  7%|▋         | 6/84 [01:31<19:43, 15.17s/it]  8%|▊         | 7/84 [01:46<19:28, 15.17s/it] 10%|▉         | 8/84 [02:01<19:12, 15.17s/it] 11%|█         | 9/84 [02:16<18:56, 15.16s/it] 12%|█▏        | 10/84 [02:31<18:41, 15.15s/it] 13%|█▎        | 11/84 [02:46<18:24, 15.14s/it] 14%|█▍        | 12/84 [03:01<18:08, 15.12s/it] 15%|█▌        | 13/84 [03:17<17:53, 15.12s/it] 17%|█▋        | 14/84 [03:32<17:38, 15.13s/it] 18%|█▊        | 15/84 [03:47<17:23, 15.12s/it] 19%|█▉        | 16/84 [04:02<17:08, 15.12s/it] 20%|██        | 17/84 [04:17<16:54, 15.14s/it] 21%|██▏       | 18/84 [04:32<16:39, 15.14s/it] 23%|██▎       | 19/84 [04:47<16:23, 15.14s/it] 24%|██▍       | 20/84 [05:03<16:09, 15.15s/it] 25%|██▌       | 21/84 [05:18<15:53, 15.13s/it] 26%|██▌       | 22/84 [05:33<15:38, 15.13s/it] 27%|██▋       | 23/84 [05:48<15:21, 15.11s/it] 29%|██▊       | 24/84 [06:03<15:07, 15.13s/it] 30%|██▉       | 25/84 [06:18<14:52, 15.13s/it] 31%|███       | 26/84 [06:33<14:36, 15.12s/it] 32%|███▏      | 27/84 [06:48<14:21, 15.11s/it] 33%|███▎      | 28/84 [07:03<14:05, 15.10s/it] 35%|███▍      | 29/84 [07:19<13:50, 15.10s/it] 36%|███▌      | 30/84 [07:34<13:34, 15.09s/it] 37%|███▋      | 31/84 [07:49<13:20, 15.10s/it] 38%|███▊      | 32/84 [08:04<13:05, 15.11s/it] 39%|███▉      | 33/84 [08:19<12:50, 15.11s/it] 40%|████      | 34/84 [08:34<12:35, 15.12s/it] 42%|████▏     | 35/84 [08:49<12:20, 15.12s/it] 43%|████▎     | 36/84 [09:04<12:07, 15.15s/it] 44%|████▍     | 37/84 [09:20<11:51, 15.14s/it] 45%|████▌     | 38/84 [09:35<11:36, 15.15s/it] 46%|████▋     | 39/84 [09:50<11:21, 15.15s/it] 48%|████▊     | 40/84 [10:05<11:06, 15.15s/it] 49%|████▉     | 41/84 [10:20<10:51, 15.15s/it] 50%|█████     | 42/84 [10:35<10:36, 15.16s/it] 51%|█████     | 43/84 [10:50<10:20, 15.14s/it] 52%|█████▏    | 44/84 [11:06<10:05, 15.15s/it] 54%|█████▎    | 45/84 [11:21<09:50, 15.14s/it] 55%|█████▍    | 46/84 [11:36<09:34, 15.13s/it] 56%|█████▌    | 47/84 [11:51<09:19, 15.12s/it] 57%|█████▋    | 48/84 [12:06<09:04, 15.13s/it] 58%|█████▊    | 49/84 [12:21<08:49, 15.12s/it] 60%|█████▉    | 50/84 [12:36<08:33, 15.11s/it] 61%|██████    | 51/84 [12:51<08:19, 15.14s/it] 62%|██████▏   | 52/84 [13:07<08:04, 15.14s/it] 63%|██████▎   | 53/84 [13:22<07:49, 15.15s/it] 64%|██████▍   | 54/84 [13:37<07:34, 15.14s/it] 65%|██████▌   | 55/84 [13:52<07:18, 15.13s/it] 67%|██████▋   | 56/84 [14:07<07:03, 15.13s/it] 68%|██████▊   | 57/84 [14:22<06:48, 15.13s/it] 69%|██████▉   | 58/84 [14:37<06:33, 15.12s/it] 70%|███████   | 59/84 [14:52<06:17, 15.12s/it] 71%|███████▏  | 60/84 [15:08<06:02, 15.11s/it] 73%|███████▎  | 61/84 [15:23<05:47, 15.12s/it] 74%|███████▍  | 62/84 [15:38<05:32, 15.13s/it] 75%|███████▌  | 63/84 [15:53<05:17, 15.14s/it] 76%|███████▌  | 64/84 [16:08<05:03, 15.16s/it] 77%|███████▋  | 65/84 [16:23<04:47, 15.14s/it] 79%|███████▊  | 66/84 [16:38<04:32, 15.14s/it] 80%|███████▉  | 67/84 [16:54<04:17, 15.13s/it] 81%|████████  | 68/84 [17:09<04:01, 15.12s/it] 82%|████████▏ | 69/84 [17:24<03:46, 15.11s/it] 83%|████████▎ | 70/84 [17:39<03:31, 15.12s/it] 85%|████████▍ | 71/84 [17:54<03:16, 15.13s/it] 86%|████████▌ | 72/84 [18:09<03:01, 15.13s/it] 87%|████████▋ | 73/84 [18:24<02:46, 15.12s/it] 88%|████████▊ | 74/84 [18:39<02:31, 15.14s/it] 89%|████████▉ | 75/84 [18:55<02:16, 15.13s/it] 90%|█████████ | 76/84 [19:10<02:00, 15.12s/it] 92%|█████████▏| 77/84 [19:25<01:45, 15.12s/it] 93%|█████████▎| 78/84 [19:40<01:30, 15.11s/it] 94%|█████████▍| 79/84 [19:55<01:15, 15.11s/it] 95%|█████████▌| 80/84 [20:10<01:00, 15.12s/it] 96%|█████████▋| 81/84 [20:25<00:45, 15.12s/it] 98%|█████████▊| 82/84 [20:40<00:30, 15.14s/it] 99%|█████████▉| 83/84 [20:56<00:15, 15.13s/it]100%|██████████| 84/84 [21:01<00:00, 12.18s/it]100%|██████████| 84/84 [21:01<00:00, 15.02s/it]
Some weights of the model checkpoint at microsoft/deberta-xxlarge-v2 were not used when initializing DebertaV2ForMaskedLM: ['deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']
- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-xxlarge-v2 and are newly initialized: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Found cached dataset jigsaw_toxicity_pred (/home/danielezer/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=jigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85)
Saving hidden states
Running evaluate with the following arguments:
Run Number: 7Model: 'deberta'
Prompt Number: '0'
Number of Examples: '500'
Dataset Name: 'jigsaw_toxicity_pred'
Dataset Directory: 'jigsaw'
No Data Balance: 'False'
Toxicity Threshold: '0'
Logistic regression accuracy: 0.812
CCS accuracy: 0.592
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Running generate with the following arguments:
Run Number: 7Model: 'deberta'
Prompt Number: '0'
Number of Examples: '500'
Dataset Name: 'jigsaw_toxicity_pred'
Dataset Directory: 'jigsaw'
No Data Balance: 'True'
Toxicity Threshold: '0'
Loading model
Loading dataloader
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 242.49it/s]
Loading cached processed dataset at /home/danielezer/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=jigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85/cache-c6571a49730c6301.arrow
Generating hidden states
  0%|          | 0/84 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
  1%|          | 1/84 [00:15<21:12, 15.33s/it]  2%|▏         | 2/84 [00:30<20:50, 15.25s/it]  4%|▎         | 3/84 [00:45<20:36, 15.26s/it]  5%|▍         | 4/84 [01:00<20:16, 15.21s/it]  6%|▌         | 5/84 [01:16<19:59, 15.18s/it]  7%|▋         | 6/84 [01:31<19:43, 15.17s/it]  8%|▊         | 7/84 [01:46<19:27, 15.16s/it] 10%|▉         | 8/84 [02:01<19:12, 15.16s/it] 11%|█         | 9/84 [02:16<18:57, 15.17s/it] 12%|█▏        | 10/84 [02:31<18:41, 15.16s/it] 13%|█▎        | 11/84 [02:47<18:26, 15.16s/it] 14%|█▍        | 12/84 [03:02<18:11, 15.15s/it] 15%|█▌        | 13/84 [03:17<17:55, 15.14s/it] 17%|█▋        | 14/84 [03:32<17:41, 15.16s/it] 18%|█▊        | 15/84 [03:47<17:26, 15.17s/it] 19%|█▉        | 16/84 [04:02<17:11, 15.17s/it] 20%|██        | 17/84 [04:18<16:57, 15.18s/it] 21%|██▏       | 18/84 [04:33<16:41, 15.17s/it] 23%|██▎       | 19/84 [04:48<16:25, 15.17s/it] 24%|██▍       | 20/84 [05:03<16:10, 15.16s/it] 25%|██▌       | 21/84 [05:18<15:54, 15.16s/it] 26%|██▌       | 22/84 [05:33<15:38, 15.14s/it] 27%|██▋       | 23/84 [05:48<15:23, 15.14s/it] 29%|██▊       | 24/84 [06:04<15:07, 15.13s/it] 30%|██▉       | 25/84 [06:19<14:52, 15.12s/it] 31%|███       | 26/84 [06:34<14:38, 15.15s/it] 32%|███▏      | 27/84 [06:49<14:22, 15.13s/it] 33%|███▎      | 28/84 [07:04<14:08, 15.14s/it] 35%|███▍      | 29/84 [07:19<13:53, 15.15s/it] 36%|███▌      | 30/84 [07:34<13:37, 15.14s/it] 37%|███▋      | 31/84 [07:49<13:22, 15.14s/it] 38%|███▊      | 32/84 [08:05<13:07, 15.14s/it] 39%|███▉      | 33/84 [08:20<12:52, 15.14s/it] 40%|████      | 34/84 [08:35<12:37, 15.15s/it] 42%|████▏     | 35/84 [08:50<12:21, 15.13s/it] 43%|████▎     | 36/84 [09:05<12:06, 15.14s/it] 44%|████▍     | 37/84 [09:20<11:51, 15.14s/it] 45%|████▌     | 38/84 [09:36<11:36, 15.15s/it] 46%|████▋     | 39/84 [09:51<11:21, 15.15s/it] 48%|████▊     | 40/84 [10:06<11:06, 15.15s/it] 49%|████▉     | 41/84 [10:21<10:51, 15.15s/it] 50%|█████     | 42/84 [10:36<10:36, 15.15s/it] 51%|█████     | 43/84 [10:51<10:20, 15.13s/it] 52%|█████▏    | 44/84 [11:06<10:05, 15.13s/it] 54%|█████▎    | 45/84 [11:21<09:50, 15.14s/it] 55%|█████▍    | 46/84 [11:37<09:35, 15.14s/it] 56%|█████▌    | 47/84 [11:52<09:20, 15.15s/it] 57%|█████▋    | 48/84 [12:07<09:05, 15.15s/it] 58%|█████▊    | 49/84 [12:22<08:50, 15.16s/it] 60%|█████▉    | 50/84 [12:37<08:35, 15.17s/it] 61%|██████    | 51/84 [12:52<08:20, 15.17s/it] 62%|██████▏   | 52/84 [13:08<08:04, 15.14s/it] 63%|██████▎   | 53/84 [13:23<07:49, 15.14s/it] 64%|██████▍   | 54/84 [13:38<07:34, 15.13s/it] 65%|██████▌   | 55/84 [13:53<07:19, 15.14s/it] 67%|██████▋   | 56/84 [14:08<07:04, 15.14s/it] 68%|██████▊   | 57/84 [14:23<06:49, 15.15s/it] 69%|██████▉   | 58/84 [14:38<06:33, 15.14s/it] 70%|███████   | 59/84 [14:54<06:18, 15.14s/it] 71%|███████▏  | 60/84 [15:09<06:02, 15.12s/it] 73%|███████▎  | 61/84 [15:24<05:47, 15.11s/it] 74%|███████▍  | 62/84 [15:39<05:32, 15.13s/it] 75%|███████▌  | 63/84 [15:54<05:18, 15.15s/it] 76%|███████▌  | 64/84 [16:09<05:02, 15.15s/it] 77%|███████▋  | 65/84 [16:24<04:47, 15.14s/it] 79%|███████▊  | 66/84 [16:39<04:32, 15.13s/it] 80%|███████▉  | 67/84 [16:55<04:17, 15.14s/it] 81%|████████  | 68/84 [17:10<04:02, 15.13s/it] 82%|████████▏ | 69/84 [17:25<03:47, 15.13s/it] 83%|████████▎ | 70/84 [17:40<03:31, 15.14s/it] 85%|████████▍ | 71/84 [17:55<03:16, 15.13s/it] 86%|████████▌ | 72/84 [18:10<03:01, 15.13s/it] 87%|████████▋ | 73/84 [18:25<02:46, 15.13s/it] 88%|████████▊ | 74/84 [18:40<02:31, 15.12s/it] 89%|████████▉ | 75/84 [18:56<02:15, 15.10s/it] 90%|█████████ | 76/84 [19:11<02:01, 15.13s/it] 92%|█████████▏| 77/84 [19:26<01:45, 15.13s/it] 93%|█████████▎| 78/84 [19:41<01:30, 15.13s/it] 94%|█████████▍| 79/84 [19:56<01:15, 15.14s/it] 95%|█████████▌| 80/84 [20:11<01:00, 15.14s/it] 96%|█████████▋| 81/84 [20:26<00:45, 15.13s/it] 98%|█████████▊| 82/84 [20:42<00:30, 15.13s/it] 99%|█████████▉| 83/84 [20:57<00:15, 15.15s/it]100%|██████████| 84/84 [21:02<00:00, 12.18s/it]100%|██████████| 84/84 [21:02<00:00, 15.03s/it]
Some weights of the model checkpoint at microsoft/deberta-xxlarge-v2 were not used when initializing DebertaV2ForMaskedLM: ['deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']
- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-xxlarge-v2 and are newly initialized: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Found cached dataset jigsaw_toxicity_pred (/home/danielezer/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=jigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85)
Saving hidden states
Running evaluate with the following arguments:
Run Number: 7Model: 'deberta'
Prompt Number: '0'
Number of Examples: '500'
Dataset Name: 'jigsaw_toxicity_pred'
Dataset Directory: 'jigsaw'
No Data Balance: 'True'
Toxicity Threshold: '0'
Logistic regression accuracy: 0.868
CCS accuracy: 0.5920000000000001
Finished running generate and evaluate with all model configurations
Copying templates file templates/jigsaw-toxicity-pred-templates.yaml to /opt/conda/lib/python3.7/site-packages/promptsource/templates/jigsaw_toxicity_pred/templates.yaml
Successfully copied templates file
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Running generate with the following arguments:
Run Number: 8Model: 'deberta'
Prompt Number: '0'
Number of Examples: '500'
Dataset Name: 'jigsaw_toxicity_pred'
Dataset Directory: 'jigsaw'
No Data Balance: 'False'
Toxicity Threshold: '0'
Loading model
Loading dataloader
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 245.24it/s]
Loading cached processed dataset at /home/danielezer/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=jigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85/cache-c6571a49730c6301.arrow
Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors
Balancing Data
Generating hidden states
  0%|          | 0/84 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
  1%|          | 1/84 [00:15<21:16, 15.38s/it]  2%|▏         | 2/84 [00:30<20:55, 15.31s/it]  4%|▎         | 3/84 [00:45<20:36, 15.26s/it]  5%|▍         | 4/84 [01:01<20:18, 15.23s/it]  6%|▌         | 5/84 [01:16<20:01, 15.21s/it]  7%|▋         | 6/84 [01:31<19:45, 15.19s/it]  8%|▊         | 7/84 [01:46<19:28, 15.18s/it] 10%|▉         | 8/84 [02:01<19:12, 15.16s/it] 11%|█         | 9/84 [02:16<18:57, 15.17s/it] 12%|█▏        | 10/84 [02:31<18:42, 15.16s/it] 13%|█▎        | 11/84 [02:47<18:26, 15.15s/it] 14%|█▍        | 12/84 [03:02<18:11, 15.16s/it] 15%|█▌        | 13/84 [03:17<17:56, 15.16s/it] 17%|█▋        | 14/84 [03:32<17:42, 15.18s/it] 18%|█▊        | 15/84 [03:47<17:25, 15.16s/it] 19%|█▉        | 16/84 [04:02<17:09, 15.15s/it] 20%|██        | 17/84 [04:17<16:54, 15.13s/it] 21%|██▏       | 18/84 [04:33<16:39, 15.14s/it] 23%|██▎       | 19/84 [04:48<16:24, 15.15s/it] 24%|██▍       | 20/84 [05:03<16:09, 15.14s/it] 25%|██▌       | 21/84 [05:18<15:55, 15.16s/it] 26%|██▌       | 22/84 [05:33<15:38, 15.14s/it] 27%|██▋       | 23/84 [05:48<15:23, 15.14s/it] 29%|██▊       | 24/84 [06:03<15:07, 15.13s/it] 30%|██▉       | 25/84 [06:19<14:52, 15.13s/it] 31%|███       | 26/84 [06:34<14:38, 15.15s/it] 32%|███▏      | 27/84 [06:49<14:22, 15.13s/it] 33%|███▎      | 28/84 [07:04<14:07, 15.14s/it] 35%|███▍      | 29/84 [07:19<13:54, 15.16s/it] 36%|███▌      | 30/84 [07:34<13:38, 15.16s/it] 37%|███▋      | 31/84 [07:50<13:23, 15.17s/it] 38%|███▊      | 32/84 [08:05<13:08, 15.16s/it] 39%|███▉      | 33/84 [08:20<12:54, 15.19s/it] 40%|████      | 34/84 [08:35<12:38, 15.18s/it] 42%|████▏     | 35/84 [08:50<12:22, 15.16s/it] 43%|████▎     | 36/84 [09:05<12:06, 15.14s/it] 44%|████▍     | 37/84 [09:20<11:51, 15.13s/it] 45%|████▌     | 38/84 [09:36<11:37, 15.15s/it] 46%|████▋     | 39/84 [09:51<11:22, 15.16s/it] 48%|████▊     | 40/84 [10:06<11:07, 15.16s/it] 49%|████▉     | 41/84 [10:21<10:51, 15.14s/it] 50%|█████     | 42/84 [10:36<10:35, 15.13s/it] 51%|█████     | 43/84 [10:51<10:19, 15.12s/it] 52%|█████▏    | 44/84 [11:06<10:05, 15.13s/it] 54%|█████▎    | 45/84 [11:22<09:49, 15.13s/it] 55%|█████▍    | 46/84 [11:37<09:34, 15.12s/it] 56%|█████▌    | 47/84 [11:52<09:19, 15.12s/it] 57%|█████▋    | 48/84 [12:07<09:05, 15.14s/it] 58%|█████▊    | 49/84 [12:22<08:49, 15.13s/it] 60%|█████▉    | 50/84 [12:37<08:34, 15.12s/it] 61%|██████    | 51/84 [12:52<08:19, 15.13s/it] 62%|██████▏   | 52/84 [13:08<08:04, 15.14s/it] 63%|██████▎   | 53/84 [13:23<07:49, 15.14s/it] 64%|██████▍   | 54/84 [13:38<07:34, 15.14s/it] 65%|██████▌   | 55/84 [13:53<07:19, 15.14s/it] 67%|██████▋   | 56/84 [14:08<07:04, 15.15s/it] 68%|██████▊   | 57/84 [14:23<06:48, 15.13s/it] 69%|██████▉   | 58/84 [14:38<06:33, 15.13s/it] 70%|███████   | 59/84 [14:53<06:18, 15.13s/it] 71%|███████▏  | 60/84 [15:09<06:02, 15.12s/it] 73%|███████▎  | 61/84 [15:24<05:48, 15.14s/it] 74%|███████▍  | 62/84 [15:39<05:32, 15.13s/it] 75%|███████▌  | 63/84 [15:54<05:17, 15.13s/it] 76%|███████▌  | 64/84 [16:09<05:02, 15.12s/it] 77%|███████▋  | 65/84 [16:24<04:47, 15.12s/it] 79%|███████▊  | 66/84 [16:39<04:32, 15.12s/it] 80%|███████▉  | 67/84 [16:54<04:17, 15.13s/it] 81%|████████  | 68/84 [17:10<04:01, 15.12s/it] 82%|████████▏ | 69/84 [17:25<03:46, 15.12s/it] 83%|████████▎ | 70/84 [17:40<03:31, 15.11s/it] 85%|████████▍ | 71/84 [17:55<03:16, 15.13s/it] 86%|████████▌ | 72/84 [18:10<03:01, 15.12s/it] 87%|████████▋ | 73/84 [18:25<02:46, 15.11s/it] 88%|████████▊ | 74/84 [18:40<02:31, 15.11s/it] 89%|████████▉ | 75/84 [18:55<02:16, 15.12s/it] 90%|█████████ | 76/84 [19:11<02:01, 15.13s/it] 92%|█████████▏| 77/84 [19:26<01:45, 15.13s/it] 93%|█████████▎| 78/84 [19:41<01:30, 15.12s/it] 94%|█████████▍| 79/84 [19:56<01:15, 15.12s/it] 95%|█████████▌| 80/84 [20:11<01:00, 15.14s/it] 96%|█████████▋| 81/84 [20:26<00:45, 15.15s/it] 98%|█████████▊| 82/84 [20:41<00:30, 15.13s/it] 99%|█████████▉| 83/84 [20:57<00:15, 15.14s/it]100%|██████████| 84/84 [21:02<00:00, 12.18s/it]100%|██████████| 84/84 [21:02<00:00, 15.03s/it]
Some weights of the model checkpoint at microsoft/deberta-xxlarge-v2 were not used when initializing DebertaV2ForMaskedLM: ['deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']
- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-xxlarge-v2 and are newly initialized: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Found cached dataset jigsaw_toxicity_pred (/home/danielezer/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=jigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85)
Saving hidden states
Running evaluate with the following arguments:
Run Number: 8Model: 'deberta'
Prompt Number: '0'
Number of Examples: '500'
Dataset Name: 'jigsaw_toxicity_pred'
Dataset Directory: 'jigsaw'
No Data Balance: 'False'
Toxicity Threshold: '0'
Logistic regression accuracy: 0.82
CCS accuracy: 0.636
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Running generate with the following arguments:
Run Number: 8Model: 'deberta'
Prompt Number: '0'
Number of Examples: '500'
Dataset Name: 'jigsaw_toxicity_pred'
Dataset Directory: 'jigsaw'
No Data Balance: 'True'
Toxicity Threshold: '0'
Loading model
Loading dataloader
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 248.58it/s]
Loading cached processed dataset at /home/danielezer/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=jigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85/cache-c6571a49730c6301.arrow
Token indices sequence length is longer than the specified maximum sequence length for this model (571 > 512). Running this sequence through the model will result in indexing errors
Generating hidden states
  0%|          | 0/84 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
  1%|          | 1/84 [00:15<21:15, 15.36s/it]  2%|▏         | 2/84 [00:30<20:52, 15.27s/it]  4%|▎         | 3/84 [00:45<20:32, 15.22s/it]  5%|▍         | 4/84 [01:00<20:15, 15.19s/it]  6%|▌         | 5/84 [01:16<19:58, 15.17s/it]  7%|▋         | 6/84 [01:31<19:42, 15.16s/it]  8%|▊         | 7/84 [01:46<19:27, 15.16s/it] 10%|▉         | 8/84 [02:01<19:12, 15.16s/it] 11%|█         | 9/84 [02:16<18:58, 15.18s/it] 12%|█▏        | 10/84 [02:31<18:44, 15.20s/it] 13%|█▎        | 11/84 [02:47<18:28, 15.18s/it] 14%|█▍        | 12/84 [03:02<18:12, 15.17s/it] 15%|█▌        | 13/84 [03:17<17:56, 15.16s/it] 17%|█▋        | 14/84 [03:32<17:39, 15.14s/it] 18%|█▊        | 15/84 [03:47<17:24, 15.14s/it] 19%|█▉        | 16/84 [04:02<17:09, 15.13s/it] 20%|██        | 17/84 [04:17<16:53, 15.13s/it] 21%|██▏       | 18/84 [04:32<16:38, 15.13s/it] 23%|██▎       | 19/84 [04:48<16:25, 15.17s/it] 24%|██▍       | 20/84 [05:03<16:10, 15.17s/it] 25%|██▌       | 21/84 [05:18<15:55, 15.16s/it] 26%|██▌       | 22/84 [05:33<15:39, 15.16s/it] 27%|██▋       | 23/84 [05:48<15:24, 15.15s/it] 29%|██▊       | 24/84 [06:04<15:09, 15.16s/it] 30%|██▉       | 25/84 [06:19<14:54, 15.17s/it] 31%|███       | 26/84 [06:34<14:41, 15.20s/it] 32%|███▏      | 27/84 [06:49<14:24, 15.17s/it] 33%|███▎      | 28/84 [07:04<14:08, 15.15s/it] 35%|███▍      | 29/84 [07:19<13:53, 15.15s/it] 36%|███▌      | 30/84 [07:34<13:37, 15.14s/it] 37%|███▋      | 31/84 [07:50<13:22, 15.14s/it] 38%|███▊      | 32/84 [08:05<13:07, 15.15s/it] 39%|███▉      | 33/84 [08:20<12:53, 15.16s/it] 40%|████      | 34/84 [08:35<12:37, 15.15s/it] 42%|████▏     | 35/84 [08:50<12:22, 15.14s/it] 43%|████▎     | 36/84 [09:05<12:06, 15.14s/it] 44%|████▍     | 37/84 [09:21<11:52, 15.16s/it] 45%|████▌     | 38/84 [09:36<11:36, 15.14s/it] 46%|████▋     | 39/84 [09:51<11:21, 15.14s/it] 48%|████▊     | 40/84 [10:06<11:06, 15.15s/it] 49%|████▉     | 41/84 [10:21<10:51, 15.15s/it] 50%|█████     | 42/84 [10:36<10:35, 15.14s/it] 51%|█████     | 43/84 [10:51<10:20, 15.14s/it] 52%|█████▏    | 44/84 [11:06<10:05, 15.13s/it] 54%|█████▎    | 45/84 [11:22<09:50, 15.13s/it] 55%|█████▍    | 46/84 [11:37<09:35, 15.13s/it] 56%|█████▌    | 47/84 [11:52<09:19, 15.13s/it] 57%|█████▋    | 48/84 [12:07<09:04, 15.12s/it] 58%|█████▊    | 49/84 [12:22<08:49, 15.12s/it] 60%|█████▉    | 50/84 [12:37<08:34, 15.13s/it] 61%|██████    | 51/84 [12:52<08:19, 15.12s/it] 62%|██████▏   | 52/84 [13:08<08:04, 15.14s/it] 63%|██████▎   | 53/84 [13:23<07:49, 15.16s/it] 64%|██████▍   | 54/84 [13:38<07:34, 15.15s/it] 65%|██████▌   | 55/84 [13:53<07:18, 15.13s/it] 67%|██████▋   | 56/84 [14:08<07:03, 15.13s/it] 68%|██████▊   | 57/84 [14:23<06:48, 15.14s/it] 69%|██████▉   | 58/84 [14:38<06:33, 15.14s/it] 70%|███████   | 59/84 [14:53<06:18, 15.14s/it] 71%|███████▏  | 60/84 [15:09<06:03, 15.13s/it] 73%|███████▎  | 61/84 [15:24<05:48, 15.13s/it] 74%|███████▍  | 62/84 [15:39<05:33, 15.14s/it] 75%|███████▌  | 63/84 [15:54<05:17, 15.12s/it] 76%|███████▌  | 64/84 [16:09<05:02, 15.13s/it] 77%|███████▋  | 65/84 [16:24<04:47, 15.14s/it] 79%|███████▊  | 66/84 [16:40<04:33, 15.17s/it] 80%|███████▉  | 67/84 [16:55<04:17, 15.17s/it] 81%|████████  | 68/84 [17:10<04:02, 15.18s/it] 82%|████████▏ | 69/84 [17:25<03:47, 15.16s/it] 83%|████████▎ | 70/84 [17:40<03:32, 15.15s/it] 85%|████████▍ | 71/84 [17:55<03:17, 15.16s/it] 86%|████████▌ | 72/84 [18:10<03:01, 15.16s/it] 87%|████████▋ | 73/84 [18:26<02:46, 15.15s/it] 88%|████████▊ | 74/84 [18:41<02:31, 15.16s/it] 89%|████████▉ | 75/84 [18:56<02:16, 15.16s/it] 90%|█████████ | 76/84 [19:11<02:01, 15.16s/it] 92%|█████████▏| 77/84 [19:26<01:46, 15.16s/it] 93%|█████████▎| 78/84 [19:41<01:30, 15.14s/it] 94%|█████████▍| 79/84 [19:57<01:15, 15.15s/it] 95%|█████████▌| 80/84 [20:12<01:00, 15.15s/it] 96%|█████████▋| 81/84 [20:27<00:45, 15.17s/it] 98%|█████████▊| 82/84 [20:42<00:30, 15.16s/it] 99%|█████████▉| 83/84 [20:57<00:15, 15.15s/it]100%|██████████| 84/84 [21:02<00:00, 12.19s/it]100%|██████████| 84/84 [21:03<00:00, 15.04s/it]
Some weights of the model checkpoint at microsoft/deberta-xxlarge-v2 were not used when initializing DebertaV2ForMaskedLM: ['deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']
- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-xxlarge-v2 and are newly initialized: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Found cached dataset jigsaw_toxicity_pred (/home/danielezer/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=jigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85)
Saving hidden states
Running evaluate with the following arguments:
Run Number: 8Model: 'deberta'
Prompt Number: '0'
Number of Examples: '500'
Dataset Name: 'jigsaw_toxicity_pred'
Dataset Directory: 'jigsaw'
No Data Balance: 'True'
Toxicity Threshold: '0'
Logistic regression accuracy: 0.908
CCS accuracy: 0.548
Finished running generate and evaluate with all model configurations
Copying templates file templates/jigsaw-toxicity-pred-templates.yaml to /opt/conda/lib/python3.7/site-packages/promptsource/templates/jigsaw_toxicity_pred/templates.yaml
Successfully copied templates file
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Running generate with the following arguments:
Run Number: 9Model: 'deberta'
Prompt Number: '0'
Number of Examples: '500'
Dataset Name: 'jigsaw_toxicity_pred'
Dataset Directory: 'jigsaw'
No Data Balance: 'False'
Toxicity Threshold: '0'
Loading model
Loading dataloader
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 252.40it/s]
Loading cached processed dataset at /home/danielezer/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=jigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85/cache-c6571a49730c6301.arrow
Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors
Balancing Data
Generating hidden states
  0%|          | 0/84 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
  1%|          | 1/84 [00:15<21:10, 15.31s/it]  2%|▏         | 2/84 [00:30<20:51, 15.26s/it]  4%|▎         | 3/84 [00:45<20:34, 15.23s/it]  5%|▍         | 4/84 [01:00<20:16, 15.21s/it]  6%|▌         | 5/84 [01:16<20:00, 15.20s/it]  7%|▋         | 6/84 [01:31<19:42, 15.17s/it]  8%|▊         | 7/84 [01:46<19:26, 15.15s/it] 10%|▉         | 8/84 [02:01<19:11, 15.15s/it] 11%|█         | 9/84 [02:16<18:56, 15.15s/it] 12%|█▏        | 10/84 [02:31<18:40, 15.14s/it] 13%|█▎        | 11/84 [02:46<18:25, 15.15s/it] 14%|█▍        | 12/84 [03:01<18:09, 15.13s/it] 15%|█▌        | 13/84 [03:17<17:55, 15.14s/it] 17%|█▋        | 14/84 [03:32<17:40, 15.14s/it] 18%|█▊        | 15/84 [03:47<17:24, 15.13s/it] 19%|█▉        | 16/84 [04:02<17:08, 15.13s/it] 20%|██        | 17/84 [04:17<16:53, 15.12s/it] 21%|██▏       | 18/84 [04:32<16:37, 15.11s/it] 23%|██▎       | 19/84 [04:47<16:23, 15.12s/it] 24%|██▍       | 20/84 [05:02<16:07, 15.12s/it] 25%|██▌       | 21/84 [05:18<15:53, 15.13s/it] 26%|██▌       | 22/84 [05:33<15:37, 15.12s/it] 27%|██▋       | 23/84 [05:48<15:21, 15.11s/it] 29%|██▊       | 24/84 [06:03<15:05, 15.10s/it] 30%|██▉       | 25/84 [06:18<14:51, 15.10s/it] 31%|███       | 26/84 [06:33<14:36, 15.12s/it] 32%|███▏      | 27/84 [06:48<14:22, 15.13s/it] 33%|███▎      | 28/84 [07:03<14:06, 15.11s/it] 35%|███▍      | 29/84 [07:19<13:51, 15.13s/it] 36%|███▌      | 30/84 [07:34<13:37, 15.14s/it] 37%|███▋      | 31/84 [07:49<13:22, 15.14s/it] 38%|███▊      | 32/84 [08:04<13:07, 15.14s/it] 39%|███▉      | 33/84 [08:19<12:51, 15.13s/it] 40%|████      | 34/84 [08:34<12:36, 15.12s/it] 42%|████▏     | 35/84 [08:49<12:21, 15.13s/it] 43%|████▎     | 36/84 [09:04<12:05, 15.11s/it] 44%|████▍     | 37/84 [09:20<11:49, 15.10s/it] 45%|████▌     | 38/84 [09:35<11:34, 15.11s/it] 46%|████▋     | 39/84 [09:50<11:19, 15.10s/it] 48%|████▊     | 40/84 [10:05<11:05, 15.11s/it] 49%|████▉     | 41/84 [10:20<10:49, 15.11s/it] 50%|█████     | 42/84 [10:35<10:36, 15.14s/it] 51%|█████     | 43/84 [10:50<10:20, 15.14s/it] 52%|█████▏    | 44/84 [11:05<10:05, 15.13s/it] 54%|█████▎    | 45/84 [11:21<09:50, 15.14s/it] 55%|█████▍    | 46/84 [11:36<09:34, 15.12s/it] 56%|█████▌    | 47/84 [11:51<09:19, 15.11s/it] 57%|█████▋    | 48/84 [12:06<09:04, 15.13s/it] 58%|█████▊    | 49/84 [12:21<08:49, 15.12s/it] 60%|█████▉    | 50/84 [12:36<08:33, 15.11s/it] 61%|██████    | 51/84 [12:51<08:18, 15.10s/it] 62%|██████▏   | 52/84 [13:06<08:03, 15.11s/it] 63%|██████▎   | 53/84 [13:21<07:48, 15.12s/it] 64%|██████▍   | 54/84 [13:37<07:33, 15.12s/it] 65%|██████▌   | 55/84 [13:52<07:19, 15.15s/it] 67%|██████▋   | 56/84 [14:07<07:03, 15.14s/it] 68%|██████▊   | 57/84 [14:22<06:48, 15.13s/it] 69%|██████▉   | 58/84 [14:37<06:33, 15.13s/it] 70%|███████   | 59/84 [14:52<06:18, 15.13s/it] 71%|███████▏  | 60/84 [15:07<06:03, 15.15s/it] 73%|███████▎  | 61/84 [15:23<05:48, 15.14s/it] 74%|███████▍  | 62/84 [15:38<05:33, 15.14s/it] 75%|███████▌  | 63/84 [15:53<05:17, 15.13s/it] 76%|███████▌  | 64/84 [16:08<05:02, 15.12s/it] 77%|███████▋  | 65/84 [16:23<04:47, 15.12s/it] 79%|███████▊  | 66/84 [16:38<04:32, 15.11s/it] 80%|███████▉  | 67/84 [16:53<04:16, 15.11s/it] 81%|████████  | 68/84 [17:08<04:01, 15.11s/it] 82%|████████▏ | 69/84 [17:24<03:47, 15.14s/it] 83%|████████▎ | 70/84 [17:39<03:32, 15.14s/it] 85%|████████▍ | 71/84 [17:54<03:16, 15.14s/it] 86%|████████▌ | 72/84 [18:09<03:01, 15.16s/it] 87%|████████▋ | 73/84 [18:24<02:46, 15.14s/it] 88%|████████▊ | 74/84 [18:39<02:31, 15.12s/it] 89%|████████▉ | 75/84 [18:54<02:16, 15.15s/it] 90%|█████████ | 76/84 [19:10<02:01, 15.14s/it] 92%|█████████▏| 77/84 [19:25<01:45, 15.14s/it] 93%|█████████▎| 78/84 [19:40<01:30, 15.14s/it] 94%|█████████▍| 79/84 [19:55<01:15, 15.14s/it] 95%|█████████▌| 80/84 [20:10<01:00, 15.13s/it] 96%|█████████▋| 81/84 [20:25<00:45, 15.16s/it] 98%|█████████▊| 82/84 [20:40<00:30, 15.14s/it] 99%|█████████▉| 83/84 [20:56<00:15, 15.13s/it]100%|██████████| 84/84 [21:01<00:00, 12.18s/it]100%|██████████| 84/84 [21:01<00:00, 15.02s/it]
Some weights of the model checkpoint at microsoft/deberta-xxlarge-v2 were not used when initializing DebertaV2ForMaskedLM: ['deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']
- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-xxlarge-v2 and are newly initialized: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Found cached dataset jigsaw_toxicity_pred (/home/danielezer/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=jigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85)
Saving hidden states
Running evaluate with the following arguments:
Run Number: 9Model: 'deberta'
Prompt Number: '0'
Number of Examples: '500'
Dataset Name: 'jigsaw_toxicity_pred'
Dataset Directory: 'jigsaw'
No Data Balance: 'False'
Toxicity Threshold: '0'
Logistic regression accuracy: 0.752
CCS accuracy: 0.596
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Running generate with the following arguments:
Run Number: 9Model: 'deberta'
Prompt Number: '0'
Number of Examples: '500'
Dataset Name: 'jigsaw_toxicity_pred'
Dataset Directory: 'jigsaw'
No Data Balance: 'True'
Toxicity Threshold: '0'
Loading model
Loading dataloader
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 246.53it/s]
Loading cached processed dataset at /home/danielezer/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-data_dir=jigsaw/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85/cache-c6571a49730c6301.arrow
Token indices sequence length is longer than the specified maximum sequence length for this model (576 > 512). Running this sequence through the model will result in indexing errors
Generating hidden states
  0%|          | 0/84 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
  1%|          | 1/84 [00:15<21:12, 15.33s/it]  2%|▏         | 2/84 [00:30<20:49, 15.24s/it]  4%|▎         | 3/84 [00:45<20:33, 15.23s/it]  5%|▍         | 4/84 [01:00<20:17, 15.21s/it]  6%|▌         | 5/84 [01:16<19:59, 15.19s/it]  7%|▋         | 6/84 [01:31<19:43, 15.18s/it]  8%|▊         | 7/84 [01:46<19:29, 15.18s/it] 10%|▉         | 8/84 [02:01<19:15, 15.20s/it] 11%|█         | 9/84 [02:16<18:58, 15.18s/it] 12%|█▏        | 10/84 [02:32<18:45, 15.20s/it] 13%|█▎        | 11/84 [02:47<18:28, 15.18s/it] 14%|█▍        | 12/84 [03:02<18:11, 15.16s/it] 15%|█▌        | 13/84 [03:17<17:56, 15.17s/it] 17%|█▋        | 14/84 [03:32<17:41, 15.16s/it] 18%|█▊        | 15/84 [03:47<17:25, 15.15s/it] 19%|█▉        | 16/84 [04:02<17:09, 15.15s/it] 20%|██        | 17/84 [04:17<16:54, 15.14s/it] 21%|██▏       | 18/84 [04:33<16:38, 15.13s/it] 23%|██▎       | 19/84 [04:48<16:24, 15.14s/it] 24%|██▍       | 20/84 [05:03<16:08, 15.13s/it] 25%|██▌       | 21/84 [05:18<15:54, 15.14s/it] 26%|██▌       | 22/84 [05:33<15:38, 15.14s/it] 27%|██▋       | 23/84 [05:48<15:23, 15.15s/it] 29%|██▊       | 24/84 [06:03<15:09, 15.15s/it] 30%|██▉       | 25/84 [06:19<14:53, 15.15s/it] 31%|███       | 26/84 [06:34<14:38, 15.15s/it] 32%|███▏      | 27/84 [06:49<14:24, 15.16s/it] 33%|███▎      | 28/84 [07:04<14:08, 15.15s/it] 35%|███▍      | 29/84 [07:19<13:53, 15.16s/it] 36%|███▌      | 30/84 [07:34<13:38, 15.15s/it] 37%|███▋      | 31/84 [07:50<13:22, 15.14s/it] 38%|███▊      | 32/84 [08:05<13:08, 15.16s/it] 39%|███▉      | 33/84 [08:20<12:53, 15.17s/it] 40%|████      | 34/84 [08:35<12:37, 15.16s/it] 42%|████▏     | 35/84 [08:50<12:22, 15.15s/it] 43%|████▎     | 36/84 [09:05<12:07, 15.15s/it] 44%|████▍     | 37/84 [09:20<11:51, 15.14s/it] 45%|████▌     | 38/84 [09:36<11:35, 15.13s/it] 46%|████▋     | 39/84 [09:51<11:20, 15.13s/it] 48%|████▊     | 40/84 [10:06<11:05, 15.13s/it] 49%|████▉     | 41/84 [10:21<10:50, 15.13s/it] 50%|█████     | 42/84 [10:36<10:35, 15.13s/it] 51%|█████     | 43/84 [10:51<10:20, 15.13s/it] 52%|█████▏    | 44/84 [11:06<10:04, 15.12s/it] 54%|█████▎    | 45/84 [11:22<09:50, 15.15s/it] 55%|█████▍    | 46/84 [11:37<09:35, 15.14s/it] 56%|█████▌    | 47/84 [11:52<09:20, 15.16s/it] 57%|█████▋    | 48/84 [12:07<09:05, 15.15s/it] 58%|█████▊    | 49/84 [12:22<08:50, 15.15s/it] 60%|█████▉    | 50/84 [12:37<08:34, 15.14s/it] 61%|██████    | 51/84 [12:52<08:19, 15.14s/it] 62%|██████▏   | 52/84 [13:08<08:04, 15.13s/it] 63%|██████▎   | 53/84 [13:23<07:49, 15.14s/it] 64%|██████▍   | 54/84 [13:38<07:34, 15.14s/it] 65%|██████▌   | 55/84 [13:53<07:19, 15.16s/it] 67%|██████▋   | 56/84 [14:08<07:04, 15.16s/it] 68%|██████▊   | 57/84 [14:23<06:48, 15.14s/it] 69%|██████▉   | 58/84 [14:38<06:34, 15.16s/it] 70%|███████   | 59/84 [14:54<06:19, 15.17s/it] 71%|███████▏  | 60/84 [15:09<06:04, 15.17s/it] 73%|███████▎  | 61/84 [15:24<05:48, 15.16s/it] 74%|███████▍  | 62/84 [15:39<05:33, 15.17s/it] 75%|███████▌  | 63/84 [15:54<05:18, 15.18s/it] 76%|███████▌  | 64/84 [16:10<05:03, 15.18s/it] 77%|███████▋  | 65/84 [16:25<04:48, 15.17s/it] 79%|███████▊  | 66/84 [16:40<04:32, 15.17s/it] 80%|███████▉  | 67/84 [16:55<04:17, 15.16s/it] 81%|████████  | 68/84 [17:10<04:02, 15.17s/it] 82%|████████▏ | 69/84 [17:25<03:47, 15.16s/it] 83%|████████▎ | 70/84 [17:40<03:31, 15.14s/it] 85%|████████▍ | 71/84 [17:56<03:16, 15.13s/it] 86%|████████▌ | 72/84 [18:11<03:01, 15.14s/it] 87%|████████▋ | 73/84 [18:26<02:46, 15.13s/it] 88%|████████▊ | 74/84 [18:41<02:31, 15.14s/it] 89%|████████▉ | 75/84 [18:56<02:16, 15.14s/it] 90%|█████████ | 76/84 [19:11<02:01, 15.15s/it] 92%|█████████▏| 77/84 [19:26<01:46, 15.16s/it] 93%|█████████▎| 78/84 [19:42<01:30, 15.16s/it] 94%|█████████▍| 79/84 [19:57<01:15, 15.16s/it] 95%|█████████▌| 80/84 [20:12<01:00, 15.17s/it] 96%|█████████▋| 81/84 [20:27<00:45, 15.15s/it] 98%|█████████▊| 82/84 [20:42<00:30, 15.14s/it] 99%|█████████▉| 83/84 [20:57<00:15, 15.12s/it]100%|██████████| 84/84 [21:03<00:00, 12.17s/it]100%|██████████| 84/84 [21:03<00:00, 15.04s/it]
Saving hidden states
Running evaluate with the following arguments:
Run Number: 9Model: 'deberta'
Prompt Number: '0'
Number of Examples: '500'
Dataset Name: 'jigsaw_toxicity_pred'
Dataset Directory: 'jigsaw'
No Data Balance: 'True'
Toxicity Threshold: '0'
Logistic regression accuracy: 0.844
CCS accuracy: 0.52
Finished running generate and evaluate with all model configurations
Finished all runs
